{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ANUNrg0tGBJL"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# <center>Codes for Homework4_IFT6390(Kaggle Competition)</center>\n",
    "\n",
    "<center>Qiang Ye (20139927),  Lifeng Wan (20108546),  Jinfang, Luo(20111308)</center>\n",
    "\n",
    "<center>Team Name on Kaggle: YLW_IFT6390_UdeM</center>\n",
    "\n",
    "<center>Last Modified: 1 december 2018</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gd-QtXrbTnPl"
   },
   "source": [
    "## Introduction\n",
    "This .ipynb is for the Kaggle in-class competition: \"Quick Draw\". \n",
    "\n",
    "The codes is designed to run on google Colab with GPU computation platform, yet it also works on local machine with only CPU supported. \n",
    "\n",
    "Before running this .ipynb file, you should download the training and testing data and put them in a folder:\n",
    "\n",
    "1. go to https://www.kaggle.com/c/ift3395-6390-f2018, download 4 data files: `train_images.npy`, `train_labels.csv`, `test_images.npy`, and `sample_submission.csv`, \n",
    "2. create a new folder named \"input\" in the directory where this .ipynb file is, \n",
    "3. put the 4 files into the folder created.\n",
    "\n",
    "`pytorch 0.4.1` is chosen as our machine learning modules; besides, codes are also relied on the following basic python libraries:\n",
    " - python 3.5.2, \n",
    " - numpy 1.14.2 \n",
    " - matplotlib 2.2.0, \n",
    " - torchvision 0.2.1\n",
    " \n",
    "Some other modules are also used yet not fully listed here.\n",
    "\n",
    "The whole .ipynb is divided into the following 11 sections:\n",
    "\n",
    "- Support for Codes Running on Colab with GPU\n",
    "- Necessary Modules & Basic Tool Functions\n",
    "- Loading Original Data\n",
    "- Baseline model - Linear SVM\n",
    "- Data Pre-Processing\n",
    "- Data Augmentation\n",
    "- Models\n",
    "- Preparation for Training Competitative Models\n",
    "- Training Models\n",
    "- Performance Analysis\n",
    "- Testing Models on Test Dataset\n",
    "\n",
    "The first section gives us an access to use the Colab platform with high speed computation. If you encounter errors when running codes in this section, the following link could help you:\n",
    "- https://colab.research.google.com/\n",
    "- https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d\n",
    "\n",
    "The second section imports basic modules required and use GPU resources if available. If error occurs in this section, it is probably because some modules are not installed. If GPU is not available, CPU will be used. \n",
    "\n",
    "From the thrid section until to the end of the file, we built our codes by first implementing some methods, then use some codes to test and show the outputs of the methods to make sure these methods are well functioned. A global bool variant `SHOW_DEMO` is used to open or close these code blocks which is somehow like:\n",
    "\n",
    "    if USE_DEMO:\n",
    "        # codes here\n",
    "        pass\n",
    "        \n",
    "We recommend you set `SHOW_DEMO = True` for the first time running this .ipynb, such that you have a good understanding of the role of each method we implemented. After you have a clear idea of all the methods, you may use `SHOW_DEMO = False` to skip all check codes, focusing on the section: **Training Models**, select **run before** in your notebook menus, and start  training a model.\n",
    "\n",
    "Without further due, let's start by executing the first code cell(line):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-CE_HEj6RzB"
   },
   "outputs": [],
   "source": [
    "SHOW_DEMO = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Is8xd8tkGP5C"
   },
   "source": [
    "## 1. Support for Codes Running on Colab with GPU\n",
    "\n",
    "The following codes help to run this .ipynb on google colab computation platform. If you run this .ipynb on your local machine, ignore the codes in this section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOsxpSJ7uNdj"
   },
   "outputs": [],
   "source": [
    "# instruction of using colab python \n",
    "# https://medium.com/deep-learning-turkey/google-colab-free-gpu-tutorial-e113627b9f5d\n",
    "\n",
    "from os.path import exists\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
    "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
    "\n",
    "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFDOOfOFlosU"
   },
   "source": [
    "Mount drive on google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7668,
     "status": "ok",
     "timestamp": 1543704410826,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "MbOHEq0_0unK",
    "outputId": "09f49d8a-ebfa-4852-da92-ffbbaedb344f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "# 4/oQAe5a98SL9kZzGrY2QyFGlcPFGy4wN4gZZ1O8R4y4zXhhJWJ0KnGT4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YinWB1MDlu8L"
   },
   "source": [
    "Redirect to current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9111,
     "status": "ok",
     "timestamp": 1543704412298,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "7TLCbYar1DCO",
    "outputId": "8d321f31-73f1-40e4-d7e0-7a00a3989fee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " candidates\t\t\t hw4.ipynb\n",
      " cnn_nets.py\t\t\t HW4_ResNet.ipynb\n",
      " Codes_HW4_IFT6390_Final.ipynb\t input\n",
      "'Copy of hw4_IFT6390.ipynb'\t models\n",
      " fastai.ipynb\t\t\t Read_Me.ipynb\n",
      " HW4-11272055.ipynb\t\t'Report of Homework4_ML.ipynb'\n",
      " HW4_1127Bagging.ipynb\t\t Sophie\n",
      " HW4_1128Bagging.ipynb\t\t submissions\n",
      "'hw4 (1).ipynb'\t\t\t Untitled0.ipynb\n",
      " hw4_IFT6390.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"drive/My Drive/Colab Notebooks/\")\n",
    "#check if you are in the right directory\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8I7pTtmmGrW9"
   },
   "source": [
    "## 2. Necessary Modules & Basic Tool Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S1vDnAXsl2bv"
   },
   "source": [
    "Load modules necessary and frequently used here, try to use GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9774,
     "status": "ok",
     "timestamp": 1543704412986,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "w_DwnRlttoKN",
    "outputId": "bb6f2141-50fa-4886-d2ca-62047aa58670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import print_function, division\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from scipy.ndimage import rotate\n",
    "\n",
    "# use GPU as priority if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    use_cuda = True\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    use_cuda = False\n",
    "    \n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sycRu_5HHip6"
   },
   "source": [
    "We built two basic and very useful tool functions here: function `debug` plays similar roles as python's builtin function: `print`; function `print_progress` displays progress for some time-comsuming procudures, which provides a user-friendly user interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oSxBaflwtoKV"
   },
   "outputs": [],
   "source": [
    "# We built two functions: `debug` and `print_progress` for debugging during \n",
    "# development and show training progress respectively.\n",
    "\n",
    "DEBUG = True\n",
    "def debug(*args, **kwargs):\n",
    "    global DEBUG\n",
    "    if DEBUG:\n",
    "        print(*args, **kwargs)\n",
    "        \n",
    "\n",
    "def print_progress(i, time_elapsed = None, before = \"progress:\", after = \"\"):\n",
    "    \"\"\"show progress of process, often used in training a neural network model\n",
    "    params\n",
    "        i: progress value in [0, 1.], double\n",
    "        time_elapsed: time elapsed from progress 0.0 to current progress, double\n",
    "        before: descriptive content displayed before progress i, str\n",
    "        after: descriptive content displayed after progress i, str\n",
    "    \"\"\"\n",
    "    if time_elapsed is not None:\n",
    "        if i >= 1:\n",
    "            time_remaining = 0\n",
    "        elif 0 < i < 1:\n",
    "            time_remaining = time_elapsed * (1. - i) / i\n",
    "        else:\n",
    "            time_remaining = float('inf')\n",
    "            \n",
    "    progress_info = '{:>7.2%}'.format(i) # align right, 7 characters atmost\n",
    "    if time_elapsed is not None:\n",
    "        progress_info += ' {:.0f}m{:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60)\n",
    "        if 0 < i < 1.0:\n",
    "            progress_info += ' {:.0f}m{:.0f}s'.format(\n",
    "                time_remaining // 60, time_remaining % 60)\n",
    "\n",
    "    # display progress repeately in the same line.\n",
    "    progress_info = '\\r' + before + progress_info + after\n",
    "    sys.stdout.flush()\n",
    "    sys.stdout.write(progress_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1L0UkDQMm4nM"
   },
   "source": [
    "We also created two folders to manager the intermediate files: models and submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JV46yc-0toKX"
   },
   "outputs": [],
   "source": [
    "# build two directories for saving model and submission files.\n",
    "folders = ['./models', './submissions']\n",
    "for i in range(len(folders)):\n",
    "    if not os.path.exists(folders[i]):\n",
    "        debug(\"creat\")\n",
    "        try:\n",
    "            debug(\"creating folder: '{}'. \".format(folders[i]), end = \"\")\n",
    "            os.mkdir(folders[i])\n",
    "            debug(\"success.\")\n",
    "        except():\n",
    "            debug(\"failure.\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Oeephl0HpbP"
   },
   "source": [
    "## 3. Loading Original Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "swnvn3fNHtav"
   },
   "source": [
    "### loading image labels from a file path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBFsoOk_nj21"
   },
   "source": [
    "Method `load_label_dataset` receives a file path and returns a tuple: `image_labels, int_to_label, label_to_int`. The first variable is a `list` keeping the  true output label data of all training images; the latter two are dicts mapping a image label from `int` to `str` for better understanding the labels and computation efficiency when needed.\n",
    "\n",
    "Method `view_data_distribution` helps to view the numbers of each class in a dataset, which is sometimes a better debug tool when predicting on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AmCTOxGBo6hu"
   },
   "outputs": [],
   "source": [
    "def load_label_dataset(file_path = './input/train_labels.csv'):\n",
    "    # ensure the training labels data are accessible by the path in next line\n",
    "    image_labels_path = './input/train_labels.csv'\n",
    "    debug(\"loading training labels from file: '{}''\".format(image_labels_path))\n",
    "    image_labels_data = np.genfromtxt(image_labels_path, names=True, delimiter=',',\n",
    "                        dtype=[('Id', 'i8'), ('Category', 'S16')])\n",
    "    debug(\"size of image labels:\", image_labels_data.shape[0])\n",
    "    \n",
    "    sample_size = image_labels_data.shape[0]\n",
    "    image_labels = np.zeros((image_labels_data.shape[0], ), dtype = np.long)\n",
    "    \n",
    "    # 2 dicts store the mapping from class id(int) to labels(str), vice versa.\n",
    "    label_to_int, int_to_label = {}, {}\n",
    "    label_i = 0 # first label index\n",
    "    for i, train_label in image_labels_data:\n",
    "        train_label = train_label.decode(\"utf-8\")\n",
    "        if label_to_int.get(train_label) is None:\n",
    "            label_to_int[train_label] = label_i\n",
    "            image_labels[i] = label_i\n",
    "            label_i += 1\n",
    "        else:\n",
    "            image_labels[i] = label_to_int[train_label]\n",
    "\n",
    "    for key, value in label_to_int.items():\n",
    "        int_to_label[value] = key   \n",
    "\n",
    "    n_class = len(int_to_label) # constant. number of classes\n",
    "\n",
    "    #debug(\"first 100 samples in image_labels:\", image_labels[0:100])\n",
    "    debug(\"dict from int to label\", int_to_label)\n",
    "    debug(\"dict from label to int\", label_to_int)\n",
    "    debug(\"train labels shape:\", image_labels.shape)\n",
    "    debug(\"number of classes = \", n_class)\n",
    "    \n",
    "    return image_labels, int_to_label, label_to_int\n",
    "\n",
    "\n",
    "def view_data_distribution(image_labels, desc, int_to_label):\n",
    "    if isinstance(image_labels, list):\n",
    "        image_labels = np.array(image_labels)\n",
    "    labels = image_labels.astype(int).reshape(-1, )\n",
    "    label_samples = np.bincount(labels)\n",
    "    sample_size = len(labels)\n",
    "    n_class = len(label_samples)\n",
    "    print(\"Distribution of: \", desc)\n",
    "    print(\"classes:{} total: {:>3} (100%) \".format(n_class, sample_size))\n",
    "    print(\"-\"*32)\n",
    "    for i in range(len(label_samples)):\n",
    "        print(\"{:>16s}: {:<3} ({:.2%})\".format(\n",
    "            int_to_label[i], label_samples[i], label_samples[i]/sample_size))\n",
    "    print(\"-\"*32)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEE0UtsT9vZq"
   },
   "outputs": [],
   "source": [
    "# ensure the training labels data are accessible by the path in next line\n",
    "if SHOW_DEMO:\n",
    "    image_labels, int_to_label, label_to_int = load_label_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKp8rt2DtoKb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    view_data_distribution(image_labels, \"image_set\", int_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9i6Egri5IqVm"
   },
   "source": [
    "### loading original training images from a file path\n",
    "\n",
    "Original image data is loaded by the following cell. Once the original image data are loaded, it will be pre-processed by noise reduction and data washing to reduce the noise in the images and remove some unreasonable images from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9KzQG5CeE8oE"
   },
   "outputs": [],
   "source": [
    "# ensure the training images data are accessible by the path in next line\n",
    "def load_image_dataset(file_path = './input/train_images.npy'):\n",
    "    debug(\"loading training images from file: '{}'... \".format(file_path))\n",
    "    images_data_original = np.load(file_path, encoding = 'latin1')\n",
    "    debug(\"successful\")\n",
    "    image_data = np.vstack(images_data_original[:, 1]) # original train X\n",
    "    X_original = image_data\n",
    "    debug(\"shape of original training X(images): \", X_original.shape)\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yaEI40X7wZc3"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    X_original = load_image_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NeJmT8kFtoKp"
   },
   "source": [
    "### visualizing image data \n",
    "\n",
    "Visualizing the images is very important for us to check if a fucntion is giving the result we expect, especially in data pre-processing and result analysis procedures. Function `show_images` receive one or a set of dataset with the same sample size and labels(if known), display all images, each from one dataset, with an image id randomly selected or user demandes. If a class_name or class id is specified, the `img_id` parameter will be ignored, and only the images of that class will be randomly selected to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h3HWHtZNtoKr"
   },
   "outputs": [],
   "source": [
    "def show_images(Xs, \n",
    "                labels = None,  \n",
    "                img_id = None, \n",
    "                class_id = None, \n",
    "                class_label = None,\n",
    "                is_test = False,\n",
    "                label_to_int = None\n",
    "               ):\n",
    "    \"\"\"show image(s) with a certain index in a list of dataset in a line.\n",
    "    Params\n",
    "        Xs: list of dataset, example: [X_original, X_resized], \n",
    "            usually all the dataset in the list have the same labels.\n",
    "        labels: labels of dataset X,  narray (size, 1) or (size, )\n",
    "        img_id: index of image to be displayed in the dataset, \n",
    "            if img_id is None, randomly select an image id from datasets.\n",
    "        class_id: specify the class_id of image will be displayed, if image\n",
    "            with img_id conflict with class_id, class_id has priority\n",
    "        class_label: name of a class, str\n",
    "        label_to_int: dict(map) from class name to class_id\n",
    "        is_test: indicating whether the image in a test image\n",
    "    \"\"\"\n",
    "    if not isinstance(Xs, list): # support single dataset.\n",
    "        Xs = [Xs]\n",
    "    if (class_id is not None or class_label is not None) and labels is None:\n",
    "        # need labels to select images with certain class_id\n",
    "        raise ValueError(\"need labels should if class_id is not None\")\n",
    "        \n",
    "    n_imgs = len(Xs) # type of images to display\n",
    "    plt.figure(figsize = (3*(n_imgs + 1), 3))\n",
    "    rnd = 0 # default image index is 0\n",
    "    rdn = img_id if img_id is not None else np.random.randint(0, Xs[0].shape[0])\n",
    "    \n",
    "    # if both id and class_id is not None, class_id has priority\n",
    "    if class_id is not None or class_label is not None:\n",
    "        class_id = class_id if class_id else label_to_int[class_label]\n",
    "        indices_with_class_id = np.where(labels == class_id)[0]\n",
    "        rdn = np.random.choice(indices_with_class_id)\n",
    "    \n",
    "    if labels is not None and is_test is False:\n",
    "        debug(\"image id:{}, class_id:{}, class_label:{}\".format(\n",
    "            rdn, labels[rdn], int_to_label[labels[rdn]]))\n",
    "    else:\n",
    "        debug(\"id:{}\".format(rdn))\n",
    "        \n",
    "    for i, X in enumerate(Xs):\n",
    "        if X is not None:\n",
    "            plt.subplot(1, n_imgs, i+1)\n",
    "            size = int(np.sqrt(X.shape[1]))\n",
    "            plt.imshow(X[rdn].reshape(size, size), cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3b7ccds_JLOV"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    show_images(X_original, image_labels, img_id = None, \n",
    "                class_label = \"squiggle\", label_to_int = label_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REoPOKFp84QA"
   },
   "source": [
    "## 4. Baseline model - Linear SVM\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dxXa9nF5s1SC"
   },
   "outputs": [],
   "source": [
    "# set this True to run the Linear SVM classifier\n",
    "RUN_LINEAR_SVC = False\n",
    "\n",
    "if RUN_LINEAR_SVC:\n",
    "    from sklearn.model_selection import train_test_split \n",
    "    from sklearn.svm import LinearSVC\n",
    "\n",
    "    train_X, test_X, train_y, test_y = train_test_split(X_original, image_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "    clf = LinearSVC(random_state=0)\n",
    "    print(clf)\n",
    "    clf.fit(train_X, train_y)  \n",
    "    pred_svm = clf.predict(test_X)\n",
    "\n",
    "    valid = np.mean(pred_svm == test_y)\n",
    "    print(\"The accuracy is {:.2f}%\".format(100*valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_PvrPWzKave"
   },
   "source": [
    "## 5. Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NZj8H2ectoKf"
   },
   "source": [
    "### noise reduction\n",
    "\n",
    "Assuming a meaningful picture will have the largest number of connected pixels(region)if it exists n a training image sample with background noises, . If we can find this largest connected points, the noise on this image can be easily removed. \n",
    "\n",
    "Several alogrithms can be used to find this largest connected pixels(region), such as deep first search(DFS) and density based scan(DBSCAN). Here, we implemented this function using both DFS and DBSCAN algorithms. For DBSCAN algorith, we chose the parameter `epsilon` = 1, and `min_samples` = 2. Both methods almost have the same effects reducing the noise of our dataset.\n",
    "\n",
    "1). using DFS to get a maximal connected point list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ca5H9S1stoKg"
   },
   "outputs": [],
   "source": [
    "# methods and class to reduce noise using DFS algorithm\n",
    "class DFS:\n",
    "    \"\"\"a deep first search class \n",
    "    \"\"\"\n",
    "    def __init__(self, my_shape, matrix): \n",
    "        \"\"\"\n",
    "        params\n",
    "            my_shape: shape of search region, tuple or list size of 2\n",
    "            matrix: search region data. narray (my_shape)\n",
    "        \"\"\"\n",
    "        self.row = my_shape[0]\n",
    "        self.col = my_shape[1]\n",
    "        self.matrix = matrix\n",
    "        \n",
    "        \n",
    "    def _is_safe(self, i, j, visited): \n",
    "        return (i >= 0 and i < self.row and \n",
    "                j >= 0 and j < self.col and \n",
    "                not visited[i, j] and self.matrix[i, j] > 0)\n",
    "    \n",
    "    \n",
    "    def get_all_neighborhood(self, i, j, visited, points):\n",
    "        \"\"\"get all points connected to point [i, j]\n",
    "        Params\n",
    "            i, j: row and col index of a given point, int, int\n",
    "            visited: mark whether a point is visited or not, narray\n",
    "            points: connected neighbors of point [i, j],list\n",
    "        Returns\n",
    "            points: \n",
    "        \"\"\"\n",
    "        points.append((i, j))\n",
    "        visited[i, j] = True\n",
    "        nbrs_row = [-1, -1, -1,  0, 0,  1, 1, 1]; \n",
    "        nbrs_col = [-1,  0,  1, -1, 1, -1, 0, 1];   \n",
    "        # Recur for all connected neighbours \n",
    "        for k in range(8): # max neighbor numbers\n",
    "            if self._is_safe(i + nbrs_row[k], j + nbrs_col[k], visited): \n",
    "                self.get_all_neighborhood(i + nbrs_row[k], j + nbrs_col[k], \n",
    "                                          visited, points)\n",
    "        return points\n",
    "    \n",
    "    \n",
    "def get_region_size(matrix, row, col):\n",
    "    \"\"\"this function changes the original data of matrix for specific purpose.\n",
    "    this function is also a recursive function.\n",
    "    \"\"\"\n",
    "    if row < 0 or col < 0 or row >= len(matrix) or col >= len(matrix[row]):\n",
    "        return 0\n",
    "    if matrix[row, col] == 0:\n",
    "        return 0\n",
    "    matrix[row, col] = 0 # avoid repeated counting\n",
    "    size = 1\n",
    "    for r in range(row - 1, row + 2):\n",
    "        for c in range(col - 1, col + 2):\n",
    "            if r != row or c != col:\n",
    "                size += get_region_size(matrix, r, c) # recursive\n",
    "    return size\n",
    "\n",
    "\n",
    "def get_point_in_biggest_region(matrix, m_shape):\n",
    "    max_region_size = 0\n",
    "    rows, cols = m_shape\n",
    "    x, y = 0, 0\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if matrix[r, c] > 0:\n",
    "                size = get_region_size(matrix, r, c)\n",
    "                if size > max_region_size:\n",
    "                    x, y = r, c\n",
    "                    max_region_size = size\n",
    "    return x, y    \n",
    "\n",
    "\n",
    "def find_max_connected_points(img, img_size = (100, 100)):\n",
    "    img_copy = copy.deepcopy(img)\n",
    "    x, y = get_point_in_biggest_region(img_copy, img_size)\n",
    "    dfs = DFS(img_size, img)\n",
    "    visited = np.zeros(img_size)\n",
    "    points = dfs.get_all_neighborhood(x, y, visited, [])\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MEr5W5uQrbLu"
   },
   "source": [
    "2). using  DBSCAN to get a maximal connected point list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2E2xyifAcrQ8"
   },
   "outputs": [],
   "source": [
    "# methods using DBSCAN to reduce noise\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def build_db_X(img, img_size = (100, 100)):\n",
    "    \"\"\"generate traing set X as a DBSCAN training set from an image with \n",
    "    certain size.\n",
    "    params\n",
    "        img: a sample from training image data, narray (100, 100) or (10000, )\n",
    "    returns\n",
    "        narray: indicating the row and col indices of non-zeor pixel, (-1, 2)\n",
    "        [[1, 2],\n",
    "         [1, 5],\n",
    "         [33, 25],\n",
    "         ...\n",
    "        ]\n",
    "        where [1, 2] is the row and col indices where a first non-zero pixel is\n",
    "    \"\"\"\n",
    "    img = img.reshape(img_size)\n",
    "    xs, ys = np.nonzero(img)\n",
    "    return np.hstack((np.array(xs).reshape(-1, 1), \n",
    "                      np.array(ys).reshape(-1, 1)))\n",
    "\n",
    "\n",
    "def find_max_connected_points_DBSCAN(img, img_size = (100, 100)):\n",
    "    \"\"\"get max connected region from a dbscan labels\n",
    "    params\n",
    "        db_X: training X for dbscan in 2D: (x, y) of non-zero pixel values in a\n",
    "              (100, 100) original images, narray (non_zero_pixel_num, 2)\n",
    "        db_labels: labels for each element of db_X, narray (non_zero_pixel_num)\n",
    "    returns\n",
    "        a list of (x, y) belong to the largest cluster, except -1(noise)\n",
    "    \"\"\"\n",
    "    db_X = build_db_X(img, img_size)\n",
    "    clustering = DBSCAN(eps = 1, min_samples = 2).fit(db_X)\n",
    "    db_labels = clustering.labels_\n",
    "    has_noise_label = True if -1 in db_labels else False\n",
    "    if has_noise_label:\n",
    "        n_clusters = len(set(db_labels)) - 1\n",
    "        # change the noise label from -1 to a positive number\n",
    "        db_labels[db_labels == -1] = n_clusters + 1\n",
    "        # np.bincount cannot handle value with negative int.\n",
    "    frequent_count = np.bincount(db_labels)\n",
    "    if has_noise_label:\n",
    "        # count for noise label is the last element of the list\n",
    "        # remove the last elment\n",
    "        frequent_count = frequent_count[0:-1] \n",
    "    # find the index of a max frequent_count value, the index is also\n",
    "    # the label value\n",
    "    label_max = np.argmax(frequent_count)\n",
    "    largest_cluster_points = []\n",
    "    for i, label in enumerate(db_labels):\n",
    "        if label == label_max:\n",
    "            largest_cluster_points.append(db_X[i,:])\n",
    "    return largest_cluster_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qNBILtZ8toKj"
   },
   "source": [
    "Once a maximal connected point list is achieved from a 2 dimensional image, the boundary of the point list is determined; thus, we uses the pixel values of the region within the boundary to get a noise-reduced image. The following methods are implemented to achieve these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cSPwhp-KtoKj"
   },
   "outputs": [],
   "source": [
    "def rebuild_image(points, old_img, img_size = (100,100)):\n",
    "    \"\"\"rebuild an noise reduced image based on the cordinates of series of max\n",
    "    connected points and old image data\n",
    "    params\n",
    "        points: a series of cordinates of points in a max connected region, list\n",
    "                [(x0, y0),(x1, y1),...]\n",
    "        old_img: old image data, narray (img_size)\n",
    "        img_size: shape of old image and of new image as well\n",
    "    returns\n",
    "        img_target: new image with the same size of old image but noise reduced\n",
    "    \"\"\"\n",
    "    old_img = old_img.reshape(img_size)\n",
    "    img_target = np.zeros_like(old_img)\n",
    "    top, down, left, right = get_bounds(points)\n",
    "    img_target[left:right+1, top:down+1] = old_img[left:right+1, top:down+1]\n",
    "    return img_target\n",
    "\n",
    "\n",
    "def get_bounds(points):\n",
    "    \"\"\"get the boundary limits of a list of point\n",
    "    \"\"\"\n",
    "    top, down, left, right = 100, 0, 100, 0\n",
    "    for x, y in points:\n",
    "        top = y if y <= top else top\n",
    "        down = y if y >= down  else down\n",
    "        left = x if x <= left else left\n",
    "        right = x if x >= right else right\n",
    "    return top, down, left, right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Un3zH1GtsqvL"
   },
   "source": [
    "Therefore, we have all the tools needed to implement a method `reduce_noise` to reduce the noise of the whole image dataset. \n",
    "\n",
    "As reducing noise is a time consuming procedure, we may not want to repeat this procedure every time we load the original image data. The natural way is to save the noise reduced data to hard drive and load it when necessary. \n",
    "\n",
    "Therefore, we built a method to load the data from a file path . if succeeded, we use the data loaded, if not, we perform `reduce_noise` to transform the original data to noise reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U37S7KrcsnaQ"
   },
   "outputs": [],
   "source": [
    "def reduce_noise(X, \n",
    "                 img_size = (100, 100),\n",
    "                 max_connected_points_func = find_max_connected_points):\n",
    "    \"\"\"reduce noise of a image dataset\n",
    "    params\n",
    "        X: a image dataset, 2D narray, (sample_size, img_pixels)\n",
    "        img_size: image shape, tuple (heigh, width)\n",
    "        max_connected_points_func: function using to find max connected points\n",
    "    returns:\n",
    "        X_copy: image dataset, shape like X, with noise reduced\n",
    "    \"\"\"\n",
    "    # some function (get_region_size) changes original data as parameters.\n",
    "    X_copy = copy.deepcopy(X)\n",
    "    debug(\"using func: '{}'\".format(\n",
    "        max_connected_points_func.__name__))\n",
    "    since = time.time()\n",
    "    for i, x in enumerate(X_copy):\n",
    "        img = x.reshape(img_size)\n",
    "        points = max_connected_points_func(img, img_size)\n",
    "        img_target = rebuild_image(points, img, img_size)\n",
    "        X_copy[i] = img_target.reshape(-1, )\n",
    "        time_elapsed = time.time() - since\n",
    "        print_progress((i+1)/len(X_copy), time_elapsed)\n",
    "\n",
    "    return X_copy\n",
    "\n",
    "\n",
    "# useful function to load data from local drive\n",
    "def data_from_file(file_path = None, \n",
    "                   description = \"\",\n",
    "                   failed_func = None,\n",
    "                   *args, **kwargs):\n",
    "    \"\"\"load numpy array data from a file path\n",
    "    params\n",
    "        file_path: where the data locates, str\n",
    "        description: brief description of the data, str\n",
    "        failed_func: if loading data failed, the function to executed\n",
    "        *args, **kwargs: parameters of the failed_func\n",
    "    returns\n",
    "        data: loaded or by failed_func. narray\n",
    "    \"\"\"\n",
    "    if file_path is None:\n",
    "        return None\n",
    "    try:\n",
    "        debug(\"loading '{}' from: '{}'... \".format(\n",
    "            description, file_path), end = \"\")\n",
    "        data = np.load(file_path, encoding = 'latin1')\n",
    "        debug(\"successful.\")\n",
    "    except:\n",
    "        debug(\"failed.\\nperforming: '{}'... \".format(\n",
    "            failed_func.__name__), end = \"\")\n",
    "        data = failed_func(*args, **kwargs)\n",
    "        debug(\"successful.\\nsaving data to: '{}'... \".format(\n",
    "            file_path), end = \"\")\n",
    "        np.save(file_path, data)\n",
    "        debug(\"complete.\")\n",
    "    debug(\"shape of '{}': {}\".format(description, data.shape))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuckeYGn_UCU"
   },
   "source": [
    "Run the following cell to see how the above methods work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0sjb7FINtoKl"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    file_X_noise_reduced = './input/noise_reduced_train_X.npy'\n",
    "    X_noise_reduced = data_from_file(file_X_noise_reduced, \n",
    "                                 \"noise reduced\",\n",
    "                                 reduce_noise,\n",
    "                                 X_original,\n",
    "                                 img_size = (100, 100)\n",
    "                                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kw5Jzz0c0PY3"
   },
   "source": [
    "This code shows the original and noise reduced forms of an random selected image from a certern image class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOXDQPl7brAZ"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    show_images([X_noise_reduced, X_original], image_labels, \n",
    "            class_label = \"screwdriver\", label_to_int = label_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmM5ajiT0zoz"
   },
   "source": [
    "This code shows the original and noise reduced forms of an image with the id = 5473."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PBkPXxIqtoKu"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    show_images([X_noise_reduced, X_original], image_labels, \n",
    "                img_id = 5473, label_to_int = label_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P5mWBM8GtoK1"
   },
   "source": [
    "### resizing image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRpcjbkH1CNw"
   },
   "source": [
    "As we can see, an noise reduced image occupies only a small area of a region with 100 × 100 pixels. Brieftly statistics show that most of the noise reduced image are within the size 30 × 30 pixels. \n",
    "\n",
    "Although we can directly feed image with size (100 × 100) to a classifier, it's much better to crop the blank area surrounding the meaningful image and resize it to a small size such as: 30 × 30. \n",
    "\n",
    "This work can be completed by the methods implemented in the following code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WgVudJk7toK2"
   },
   "outputs": [],
   "source": [
    "from skimage.util import pad\n",
    "from skimage.transform import resize\n",
    "\n",
    "def crop_img(img):\n",
    "    \"\"\"cut off the black area surrounding non-zero pixels. \n",
    "    params\n",
    "        img: image to be cropeed, 2d narray\n",
    "    returns:\n",
    "        cropped image, 2d narray\n",
    "    \"\"\"\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    return img[rmin: rmax + 1, cmin: cmax + 1]\n",
    "\n",
    "def my_resize(img, size = (30, 30)):\n",
    "    \"\"\"resized img will keep its resolution and will be centered if smaller \n",
    "    than original img.\n",
    "    \"\"\"\n",
    "    height, width = img.shape\n",
    "    ratio = max(height / size[0],  width / size[1])\n",
    "    if ratio > 1:\n",
    "        new_size = (int(img.shape[0] / ratio), int(img.shape[1] / ratio))\n",
    "        resized = resize(img, new_size, mode = 'reflect')\n",
    "    else:\n",
    "        resized = img\n",
    "    pad_left = int((size[1] - resized.shape[1])/2)\n",
    "    pad_right = size[1] - pad_left - resized.shape[1]\n",
    "    \n",
    "    pad_top =  int((size[0] - resized.shape[0])/2)\n",
    "    pad_bottom = size[0] - pad_top - resized.shape[0]\n",
    "    pads = ((pad_top, pad_bottom), (pad_left, pad_right))\n",
    "    resized = np.pad(resized, pads, 'constant', constant_values = 0)\n",
    "    return resized\n",
    "\n",
    "def resize_dataset(X, old_size = (100, 100), new_size = (30, 30)):\n",
    "    \"\"\"resize the dataset X, from old_size to new_size\n",
    "    \"\"\"\n",
    "    debug(\"resizing images... \")\n",
    "    n_size = len(X)\n",
    "    resized_X = np.zeros((n_size, new_size[0] * new_size[1]))\n",
    "    since = time.time()\n",
    "    for i, x in enumerate(X):\n",
    "        img = x.reshape(old_size)\n",
    "        img = crop_img(img)\n",
    "        resized_X[i] = my_resize(img, size = new_size).reshape(-1, )\n",
    "        time_elapsed = time.time() - since\n",
    "        print_progress((i+1)/len(X), time_elapsed)\n",
    "    return resized_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUCganKW_ehV"
   },
   "source": [
    "Run the following cell to see how the above methods work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zh6rZvr_EKxx"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    file_X_resized = './input/resized_and_noise_reduced_X.npy'\n",
    "    X_resized = data_from_file(file_X_resized, \n",
    "                           \"X_resized\",\n",
    "                           resize_dataset,\n",
    "                           X_noise_reduced,\n",
    "                           old_size = (100, 100),\n",
    "                           new_size = (30, 30)\n",
    "                          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K_FKZHBt4QN9"
   },
   "source": [
    "The following code shows the original, noise reduced and resized form of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jp8FN_KLMMCx"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    show_images([X_resized, X_noise_reduced, X_original], image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ACk9xZ2NOlaH"
   },
   "source": [
    "### normalization\n",
    "Normalization is also a good choice for data pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zzdvtOaX8dF7"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "def normalize_dataset(X):\n",
    "    debug(\"normalizing data... \", end=\"\")\n",
    "    minmax = MinMaxScaler()\n",
    "    X_norm = minmax.fit_transform(X)\n",
    "    debug(\"complete.\")\n",
    "    return X_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CaIY_fiFtoK_"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    # normalization\n",
    "    use_normalization = True\n",
    "    if use_normalization:\n",
    "        X_normed = normalize_dataset(X_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xKs44l29BGz"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    #show_images([X_normed, X_resized, X_noise_reduced, ], image_labels)\n",
    "    show_images([X_original, X_noise_reduced, X_resized, X_normed ], image_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z266AuBP9GGB"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    # now X_resized is normalized\n",
    "    X_resized = X_normed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_hS0bSBaKr8X"
   },
   "source": [
    "### merging images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jE_onyg27f9l"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    # merge train_images and labels to one numpy array\n",
    "    # very important varialbe: train_valid_set\n",
    "    file_path = './input/pre_processed_train_data.npy'\n",
    "    train_valid_set = data_from_file(file_path,\n",
    "                          \"train_valid_dataset\",\n",
    "                          np.concatenate,\n",
    "                          (X_resized, image_labels.reshape(-1, 1)),\n",
    "                          axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ovf5UsJTc9X"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    show_images([train_valid_set[:,:-1]], train_valid_set[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "atv3NzvrlIWD"
   },
   "source": [
    "### washing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsbGEsUvAJO0"
   },
   "source": [
    "Some images are meanless or have no relationship with their class labels. We may drop them. Have a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDDTCHMeX9m2"
   },
   "outputs": [],
   "source": [
    "def load_bad_sample_ids(): \n",
    "    bad_lists = [None] * 8\n",
    "\n",
    "    # 1. screwdriver peanut and spoon\n",
    "    bad_lists[0] = [42, 186, 285, 483, 541, 612, 627, 851, 1045, 1083, 1212,\n",
    "                    1245, 1438, 1598, 1820, 2199, 2262, 2361, 2517, 2522, 2737,\n",
    "                    2791, 2846, 2873, 3226, 3452, 3571, 3769, 3780, 4176, 4201,\n",
    "                    4642, 4749, 4856, 4913, 4988, 5282, 5337, 5642, 5858, 6029,\n",
    "                    6099, 6378, 6523, 6825, 7129, 7965, 8430, 8645, 8655, 9001,\n",
    "                    9389\n",
    "                   ]\n",
    "    # 2. nail and rabbit\n",
    "    bad_lists[1] = [38, 124, 128, 164, 303, 368, 384, 478, 550, 693, 727, 749,\n",
    "                    757, 932, 1093, 1133, 1205, 1214, 1251, 1388, 1400, 1421,\n",
    "                    1503, 1507, 1661, 1695, 1708, 1778, 1842, 1844, 1853, 1923,\n",
    "                    2042, 2080, 2120, 2130, 2170, 2184, 2461, 2680, 2796, 3133,\n",
    "                    3373, 3379, 3381, 3414, 4148, 4212, 4576, 4763, 4831, 5000,\n",
    "                    5249, 5372, 5447, 5500, 5546, 5967, 6141, 6209, 6283, 6291,\n",
    "                    6299, 6768, 7017, 7037, 7115, 7119, 7279, 7608, 7684, 7883,\n",
    "                    7908, 8033, 8141, 8191, 8236, 8282, 8531, 8648, 8746, 8786,\n",
    "                    8871, 9107, 9218, 9491, 9536, 9651, 9699, 9761, 9957\n",
    "                   ]\n",
    "    # bad_list_2 = []\n",
    "    # 3. panda\n",
    "    bad_lists[2] = [372, 386, 650, 924, 1333, 2865, 3873, 4018, 4049, 4257, \n",
    "                    5229, 5284, 5516, 6300, 7315, 7409, 9712, 9927\n",
    "                   ]\n",
    "    # 4. rifle\n",
    "    bad_lists[3] = [50, 292, 363, 748, 1319, 1382, 1571, 2026, 2764, 3100, 3534,\n",
    "                    3657, 4088, 4092, 4191, 4719, 4789, 4918, 5086, 5162, 5192,\n",
    "                    5404, 5989, 6246, 6502, 7013, 7327, 7349, 7557, 8765, 9445, \n",
    "                    9564, 9846, 9960\n",
    "                   ]\n",
    "    # 5. sink\n",
    "    bad_lists[4] = [2362, 3196, 3410, 4987, 5121, 5604, 6092, 6462, 7341, 9340,\n",
    "                    9817\n",
    "                   ]\n",
    "    # 6. pear\n",
    "    bad_lists[5] = [341, 4608, 5329, 7835, 7841]\n",
    "\n",
    "    # 7. moustache\n",
    "    bad_lists[6] = [474, 658, 1005, 1363, 1539, 1657, 1893, 2258, 2575, 2709,\n",
    "                    2912, 3020, 3184, 3466, 4035, 4504, 4742, 4938, 4961, 4986,\n",
    "                    6024, 6414, 6451, 6677, 7098, 7148, 7235, 7388, 7761, 8476,\n",
    "                    9737, 9855, 9980\n",
    "                   ]\n",
    "    # bad_list_7 = []\n",
    "    # 8. parrot\n",
    "    bad_lists[7] = [1221, 2028, 2167, 2387, 3309, 3517, 4142, 4747, 4821, 5873,\n",
    "                    7154, 7208, 7314, 8569, 8868, 9872\n",
    "                   ]\n",
    "    bad_sample_ids = []\n",
    "    for i in range(8):\n",
    "        bad_sample_ids.extend(bad_lists[i])\n",
    "        \n",
    "    bad_sample_ids = list(set(bad_sample_ids))\n",
    "    debug(\"{} samples to be washed out\".format(len(bad_sample_ids)))\n",
    "    return bad_sample_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZA6oE5UZXBlu"
   },
   "outputs": [],
   "source": [
    "# an image with hand written 'pear' labeld 'pear'\n",
    "if SHOW_DEMO:\n",
    "    show_images([X_resized, X_noise_reduced, X_original], image_labels, img_id = 341)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FlEkPvusjBtc"
   },
   "outputs": [],
   "source": [
    "# Pay attention to to source (which dataset) of the indices of bad samples\n",
    "def wash_samples(data_set):\n",
    "    \"\"\"\n",
    "    params\n",
    "        bat_sample_ids: [int, int, int, ...]\n",
    "    \"\"\"\n",
    "    bad_sample_ids = load_bad_sample_ids()\n",
    "    data_set_clean = data_set\n",
    "    # if you use original data to remove bad samples. do not\n",
    "    # add your code here\n",
    "    debug(\"washing out bad samples...\", end = \" \")\n",
    "    data_set_clean = np.delete(data_set, bad_sample_ids, axis = 0)\n",
    "    debug(\"complete. current data shape: {}\".format(data_set_clean.shape))\n",
    "    assert data_set.shape[0] - data_set_clean.shape[0] == len(bad_sample_ids),\\\n",
    "        \"data length mismatch before and after removing.\"\n",
    "    \n",
    "    return data_set_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10330,
     "status": "ok",
     "timestamp": 1543704413841,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "Fx4hyYZTYD9N",
    "outputId": "282f1729-14ff-4224-b44f-ec088d75de75"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfile_path = \\'./input/pre_processed_train_data.npy\\'\\ntrain_valid_set = data_from_file(file_path,\\n                          \"train_valid_dataset\",\\n                          np.concatenate,\\n                          (X_resized, image_labels.reshape(-1, 1)),\\n                          axis = 1)\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using the following code to reload train_valid_set if you wash_samples more than\n",
    "# 2 times\n",
    "'''\n",
    "file_path = './input/pre_processed_train_data.npy'\n",
    "train_valid_set = data_from_file(file_path,\n",
    "                          \"train_valid_dataset\",\n",
    "                          np.concatenate,\n",
    "                          (X_resized, image_labels.reshape(-1, 1)),\n",
    "                          axis = 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9CxbRPyX-FM"
   },
   "outputs": [],
   "source": [
    "# remove bad samples\n",
    "# ** CAUTION: **\n",
    "# This cell shouldn't be run more than 1 time, cause it may delete\n",
    "# other normal image data or delete a image with the id that donesn't exist。\n",
    "# Rebuild train_valid_set in the last cell if you unfortunately run this cell\n",
    "# twice or more\n",
    "\n",
    "if SHOW_DEMO:\n",
    "    use_bad_samples_removal = True\n",
    "    # add your bad sample ids in this list\n",
    "    if use_bad_samples_removal:\n",
    "        train_valid_set = wash_samples(train_valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KZoFvQZjBHtq"
   },
   "source": [
    "Run and re-run the following cell to see different images in current dataset\n",
    "\n",
    "It is always recommended to show some images after some operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2z9vChb9UCt4"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    show_images([train_valid_set[:,:-1]], train_valid_set[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_rT1FN1QDCH"
   },
   "source": [
    "## 6. Data Augmentation\n",
    "\n",
    "Several augmentation techniques are used in thie problem, which are: rotation, uniformed noising, flipping etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EnQXhz-ltoLG"
   },
   "source": [
    "### data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKN6PWHuBi8I"
   },
   "source": [
    "It's time to split our dataset to **train** and **validate** datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7AVkEk1klEdW"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    # change your validate set size here\n",
    "    n_valid_set = 1000\n",
    "\n",
    "    # if you do data augmentation, update data_size    \n",
    "    data_size, n_features = train_valid_set.shape\n",
    "    n_features -= 1 # cut label \n",
    "\n",
    "    valid_start = data_size - n_valid_set # numbers\n",
    "\n",
    "    # split to train_set and valid_set\n",
    "    train_set = train_valid_set[0:valid_start,:]\n",
    "    valid_set = train_valid_set[valid_start:,:]\n",
    "\n",
    "    debug(\"shape of train_set: {}\".format(train_set.shape))\n",
    "    debug(\"shape of valid set: {}\".format(valid_set.shape))\n",
    "    data_size, valid_start, train_end = None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AKSY1PvZQJcA"
   },
   "source": [
    "### rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YHP976LmtoK8"
   },
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate\n",
    "\n",
    "def ds_rotate(data_set, degree = 15, img_size = (30, 30)):\n",
    "    \"\"\"rotate an image dataset with certain degrees with an image size\n",
    "    params\n",
    "        data_set: image dataset with a label, narray (size, n_feature + 1)\n",
    "        degree: int, -360 - 360, usually use -15 - 15\n",
    "        img_size: current and rotated image size\n",
    "    returns\n",
    "        data_set_rotated: narray (size, n_feature + 1), label unchanged        \n",
    "    \"\"\"\n",
    "    data_X = data_set[:, 0:-1]\n",
    "    data_labels = data_set[:, -1]\n",
    "\n",
    "    data_X_rotated = rotate(\n",
    "        data_X.reshape(-1, img_size[0], img_size[1]),\n",
    "        degree, axes = (1,2), reshape = False)\n",
    "    data_X_rotated = data_X_rotated.reshape(\n",
    "        -1, img_size[0] * img_size[1])\n",
    "    data_set_rotated = np.concatenate(\n",
    "        (data_X_rotated, data_labels.reshape(-1, 1)), axis = 1)   \n",
    "    return data_set_rotated\n",
    "\n",
    "def augment_with_rotate(data_set):\n",
    "    \"\"\"tow degrees 15, -15 are used to augment dataset. return the image \n",
    "    rotated dataset, not include the original dataset.\n",
    "    \"\"\"\n",
    "    degrees = np.array([-15, -10, -5, 5, 10, 15])\n",
    "    new_set = None\n",
    "    i, total = 0, len(degrees)\n",
    "    s_before = \"rotating image(special)...\"\n",
    "    since = time.time()\n",
    "    for degree in degrees:\n",
    "        temp_set = ds_rotate(data_set, degree)\n",
    "        if new_set is None:\n",
    "            new_set = temp_set\n",
    "        else:\n",
    "            new_set = np.concatenate((new_set, temp_set), axis = 0)\n",
    "        \n",
    "        i += 1\n",
    "        time_elapsed = time.time() - since\n",
    "        print_progress(i/total, time_elapsed, s_before)\n",
    "\n",
    "    debug(\"complete.\")\n",
    "    return new_set\n",
    "\n",
    "def sub_set_of_class(class_label, data_set):\n",
    "    \"\"\"get a subset from dataset with all samples are of class_label\"\"\"\n",
    "    class_id = label_to_int[class_label]\n",
    "    indices_with_class_id = np.where(data_set[:,-1] == class_id)[0]\n",
    "    return data_set[indices_with_class_id, :]\n",
    "\n",
    "def augment_with_rotate2(data_set):\n",
    "    \"\"\"perform a specific augmente with certain degrees for whole dataset\n",
    "    and some degrees for images with certain labels. return the augmented\n",
    "    dataset(original dataset not included)\"\"\"\n",
    "    s_before = \"rotating image(special)...\"\n",
    "    since = time.time()\n",
    "    degrees = np.array([-15, -10, -5, 5, 10, 15])\n",
    "    special_degrees = [-150, -120, -90, -75, -60, -45, \n",
    "                        150,  120,  90,  75,  60,  45]\n",
    "    special_labels = ['peanut', 'spoon', 'shovel', 'screwdriver', 'squiggle',\n",
    "                      'pencil', 'parrot']\n",
    "    new_set = None\n",
    "    total = len(degrees) + len(special_degrees) * len(special_labels)\n",
    "    i = 0.0\n",
    "    for degree in degrees:\n",
    "        temp_set = ds_rotate(data_set, degree)\n",
    "        if new_set is None:\n",
    "            new_set = temp_set\n",
    "        else:\n",
    "            new_set = np.concatenate((new_set, temp_set), axis = 0)\n",
    "        i += 1\n",
    "        time_elapsed = time.time() - since\n",
    "        print_progress(i/total, time_elapsed, s_before)\n",
    "    #debug(new_set.shape)\n",
    "    \n",
    "    for label in special_labels:\n",
    "        sub_set = sub_set_of_class(label, data_set)\n",
    "        for degree in special_degrees:\n",
    "            temp_set = ds_rotate(sub_set, degree)\n",
    "            new_set = np.concatenate((new_set, temp_set), axis = 0)\n",
    "            i += 1\n",
    "            time_elapsed = time.time() - since\n",
    "            print_progress(i/total, time_elapsed, s_before)\n",
    "    time_elapsed = time.time() - since\n",
    "    print_progress(1, time_elapsed, s_before, ' complete')\n",
    "    print()\n",
    "    #debug(new_set.shape)\n",
    "    return new_set\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-jpB5x1oK9-"
   },
   "outputs": [],
   "source": [
    "# test for function: sub_set_of_class\n",
    "def _unit_test(train_set):\n",
    "    test_data_set = copy.deepcopy(train_set)\n",
    "    labels = int_to_label.values()#\n",
    "    total_size = 0\n",
    "    n_size, n_feature = test_data_set.shape\n",
    "    for label in labels:\n",
    "        sub_set = sub_set_of_class(label, test_data_set)\n",
    "        sub_size, n_feature = sub_set.shape\n",
    "        n_feature -= 1\n",
    "        total_size += sub_size\n",
    "        assert label_to_int[label] == np.sum(sub_set[:,-1])/sub_size,\\\n",
    "            \"method 'sub_set' may not function well. check codes.\"\n",
    "    assert n_size == total_size, \\\n",
    "        \"total size mismatch, check codes...\"\n",
    "    return True\n",
    "\n",
    "if SHOW_DEMO:\n",
    "    _unit_test(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ndLyoVrzDRAU"
   },
   "source": [
    "One of the two different rotation strategies is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "grkdC-6nWFBb"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    # run this cell only once\n",
    "    use_augment_with_rotate = True # general rotate\n",
    "    use_augment_with_rotate2 = False\n",
    "    # extra rotate for images with special labels\n",
    "    train_set_rotated = None\n",
    "    if use_augment_with_rotate:\n",
    "        train_set_rotated = augment_with_rotate(train_set)\n",
    "    if use_augment_with_rotate2:\n",
    "        train_set_rotated = augment_with_rotate2(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eplGynUYDbPM"
   },
   "source": [
    "Run the next cell to see an images rotated and its original image( if use first strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YKU58cWWtoLC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    if use_augment_with_rotate:\n",
    "        size, _ = train_set_rotated.shape # update size\n",
    "        show_images([train_set[:,:-1], \n",
    "                     train_set_rotated[:int(size/6),:-1],\n",
    "                     train_set_rotated[int(size/6):int(2*size/6),:-1],\n",
    "                     train_set_rotated[int(2*size/6):int(3*size/6),:-1],\n",
    "                     train_set_rotated[int(3*size/6):int(4*size/6),:-1],\n",
    "                     train_set_rotated[int(4*size/6):int(5*size/6),:-1],\n",
    "                     train_set_rotated[int(5*size/6):,:-1],\n",
    "                    ], train_set[:,-1])\n",
    "    if use_augment_with_rotate2:\n",
    "        show_images([train_set_rotated[:,:-1]], train_set_rotated[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dawAa-cdtoLL"
   },
   "source": [
    "### uniformly randomized noising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l60HotaLDt-p"
   },
   "source": [
    "Original noises  are reduced, some weak, randomized, uniformd background noises are imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eEadeOFSZJuO"
   },
   "outputs": [],
   "source": [
    "def augment_with_noise(data_set, use_normalization = True, density_ratio = 0.4):\n",
    "    max_v = 1 if use_normalization else 255\n",
    "    set_noised = np.random.uniform(0, density_ratio * max_v, data_set.shape)\n",
    "    set_noised[:, -1] = 0\n",
    "    set_noised += data_set\n",
    "    max_pixel_v = np.max(set_noised[:,:-1])\n",
    "    # label data not included\n",
    "    set_noised[:,:-1] = set_noised[:,:-1] * max_v / max_pixel_v\n",
    "    #set_noised = np.clip(set_noised, 0, 255)\n",
    "    #data_set_noised = np.concatenate((set_noised, data_set), axis = 0)\n",
    "    debug(\"dataset augmented with noise, \", end = \"\")\n",
    "    debug(\"current data shape: {}\".format(set_noised.shape))\n",
    "    return set_noised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SwJZWclsEHTU"
   },
   "source": [
    "Run the following two cells to see how new noises are like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hJNwpNmptoLM"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    # add uniform noise to train_data\n",
    "    # run this cell only once\n",
    "    use_augment_with_noise = True\n",
    "    train_set_noised = None\n",
    "    if use_augment_with_noise:\n",
    "        train_set_noised = augment_with_noise(train_set)\n",
    "\n",
    "    debug(train_set_noised.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPOp7upobGG4"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    if use_augment_with_noise:\n",
    "        show_images([train_set[:,:-1], train_set_noised[:,:-1]], train_set[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PwZO5rgMsfUG"
   },
   "source": [
    "### flipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3WaFI_VEPPu"
   },
   "source": [
    "Flipping images from left to right to get an mirror image of the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y0mnpJ8qtoLV"
   },
   "outputs": [],
   "source": [
    "def augment_with_fliplr(data_set, img_size = (30, 30)):\n",
    "    set_flipped = np.zeros(data_set.shape)\n",
    "    set_flipped[:, -1] = data_set[:, -1]\n",
    "    for i in range(len(data_set)):\n",
    "        x = train_set[i,0:-1].reshape(img_size)\n",
    "        set_flipped[i, 0:-1] = np.fliplr(x).reshape(1, -1)\n",
    "    debug(\"dataset augmented with fliplr. \")\n",
    "    #debug(\"current data shape: {}\".format(data_set_flipped.shape))\n",
    "    return set_flipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tohM0SmcEVTa"
   },
   "source": [
    "Run the following two cells to see how flipping works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S6fnHP1SeOT-"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    # run this cell only once\n",
    "    use_augment_with_fliplr = True\n",
    "    train_set_flipped = None\n",
    "    if use_augment_with_fliplr:\n",
    "        train_set_flipped = augment_with_fliplr(train_set)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yw1KRLgYtoLS"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    if use_augment_with_fliplr:\n",
    "        show_images([train_set[:,:-1], train_set_flipped[:,:-1]], train_set[:,-1])\n",
    "    \n",
    "#debug(train_set[0:int(train_size/2),-1] == train_set[int(train_size/2):,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cve5BPwxs5cq"
   },
   "source": [
    "## 7. Models\n",
    "\n",
    "A model base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eU6LjUheOwC6"
   },
   "outputs": [],
   "source": [
    "class MyModule(nn.Module):\n",
    "    def __init__(self, p_fc, p_conv, suffix):\n",
    "        super(MyModule, self).__init__()\n",
    "        self.dropout_p = p_fc\n",
    "        self.dropout_conv_p = p_conv\n",
    "        self.suffix = suffix\n",
    "\n",
    "    def get_name(self, suffix = None):\n",
    "        if suffix is not None:\n",
    "            self.suffix = suffix\n",
    "        name =  type(self).__name__ +  \"_\" + str(self.dropout_p) \n",
    "        name += \"_\" + str(self.dropout_conv_p) + \"_\" + str(suffix)\n",
    "        return name    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yHET5W2Xs_5B"
   },
   "source": [
    "### MLP - weak\n",
    "multi-layer perception works better than LInear SVM. Here a MLP with two hidden layer is built, and it brought us the accuracy around 50 - 60% on this problem. \n",
    "To be better? Let's try convolutional neural network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xrRsm7p3tByc"
   },
   "outputs": [],
   "source": [
    "class MLP(MyModule):\n",
    "    \"\"\"a simple MLP with 2 hidden layers\"\"\"\n",
    "    def __init__(self, p_fc = 0.0, p_conv = 0.0, suffix = \"\"):\n",
    "        super(MLP, self).__init__(p_fc, p_conv, suffix)\n",
    "        self.output = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(900, 512),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(self.dropout_p),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p = self.dropout_p),\n",
    "            \n",
    "            nn.Linear(256, n_class), \n",
    "        )\n",
    "        self.suffix = \"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mUhUF6K_s6rB"
   },
   "source": [
    "### Net5 -  a small competitative Res-CNN\n",
    "\n",
    "This is a small convolutional neural network with 5 convolutional layers with eath followed by a batch normalization layer. One shorcut path added to this network makes it a little like ResNet. 2 max pool subsampling layers are used to reduce the size of each channel. Receiving an input with the shape [size, 1, 30, 30], the network produces a tensor with the shape of [size, 128, 4, 4] after 5 convolutional layers and 2 max pool subsampling operations. Then 2 full connec layers performing linear transformations and a softmax operation are followed to produce a tensor shaped [size, 31], which is what we expect.\n",
    "\n",
    "A `get_name` method is implemented to produce a specific name for the model given some suffix, so that a network is distinguishable when save, load the model from file or make submission files.\n",
    "\n",
    "We name this network `Net5` as it is the fifth edition of CNNs we tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IJ3n78S89ZO5"
   },
   "outputs": [],
   "source": [
    "class Net5(MyModule):\n",
    "    \"\"\"CNN with best result\"\"\"\n",
    "    def __init__(self, channels = [32, 64, 128, 128, 128],\n",
    "                 p_fc = 0.0, p_conv = 0.0, hidden = 1000, suffix = \"\"):\n",
    "        super(Net5, self).__init__(p_fc, p_conv, suffix)   \n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        self.conv1 = nn.Conv2d(1, channels[0], 3, stride = 1, padding = 1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(channels[0])\n",
    "        self.conv2 = nn.Conv2d(channels[0], channels[1], 3, stride = 1,\n",
    "                               padding = 1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(channels[1])\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3)\n",
    "        self.conv3_bn = nn.BatchNorm2d(channels[2])\n",
    "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3)\n",
    "        self.conv4_bn = nn.BatchNorm2d(channels[3])\n",
    "        self.conv5 = nn.Conv2d(channels[3], channels[4], 3)\n",
    "        self.conv5_bn = nn.BatchNorm2d(channels[4])\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.fc1 = nn.Linear(channels[4]*4*4, hidden)\n",
    "        self.fc_drop = nn.Dropout(p = self.dropout_p)\n",
    "        self.fc2 = nn.Linear(hidden, n_class)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):                             # 1    30   30\n",
    "        shortcut = x\n",
    "        x = self.conv1_bn(self.relu(self.conv1(x)))   #31    30   30\n",
    "        x = self.conv2_bn(self.relu(self.conv2(x)))   #64    30   30 \n",
    "\n",
    "        x += shortcut                                 #shortcut\n",
    "        \n",
    "        x = self.conv3_bn(self.relu(self.conv3(x)))   #128   28   28  \n",
    "        x = self.maxpool(x)                           #128   14   14\n",
    "        \n",
    "        x = self.conv4_bn(self.relu(self.conv4(x)))   #128   12   12         \n",
    "        x = self.maxpool(x)                           #128    6    6 \n",
    "        \n",
    "        x = self.conv5_bn(self.relu(self.conv5(x)))   #128    4    4\n",
    "            \n",
    "        x = x.view(x.size(0), -1) # batch, 128\n",
    "        x = self.fc_drop(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oZqgX_Mu9chi"
   },
   "source": [
    "### MyResNet - works but not well tuned\n",
    "\n",
    "The idea of this ResNet comes from ResNet18 receiving 3 original channels which is not directly applicable to our problem, we therefore built a similar ResNet and called it `MyResNet`. Different from the implementation of ResNet18,  changing the value of padding is used to keep or reduce the size of each channel whereas ResNet18 in torchvision uses stride.\n",
    "\n",
    "This network works yet is not well tuned. We stopped tuning and training  this network at the accuracy about 60%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oGWfHEzvtoLl"
   },
   "outputs": [],
   "source": [
    "# a little ResNet similar to ResNet18 in torchvision, \n",
    "# semms easy to overfit data\n",
    "class MyBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, inplanes, planes, padding = 1, drop_out = 0.5,\n",
    "                 downsample = None):\n",
    "        # if padding = 0, width/height will reduce 2 after first conv\n",
    "        super(MyBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 3, stride=1, padding = padding)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, stride = 1, padding = 1)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.padding = padding\n",
    "        self.dropout = nn.Dropout2d(p = drop_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.dropout(self.bn1(out))\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.dropout(self.bn2(out))\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out    \n",
    "    \n",
    "    \n",
    "class MyResNet(MyModule):\n",
    "    def __init__(self, block, layers, channels, p_fc = 0, p_conv = 0,\n",
    "                 n_class = 31, suffix = \"\"):\n",
    "        super(MyResNet, self).__init__(p_fc, p_conv, suffix)\n",
    "        self.inplanes = channels[0]\n",
    "        self.layer0 = nn.Sequential(\n",
    "            nn.Conv2d(1, channels[0], 3, stride = 1, padding = 0, bias = False),\n",
    "            nn.BatchNorm2d(channels[0]),\n",
    "            nn.ReLU(inplace = True),\n",
    "            # nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.maxpool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block,channels[1],layers[0], padding=1)\n",
    "        self.layer2 = self._make_layer(block,channels[2],layers[1], padding=0)\n",
    "        self.layer3 = self._make_layer(block,channels[3],layers[2], padding=1)\n",
    "        self.layer4 = self._make_layer(block,channels[4],layers[3], padding=1)\n",
    "\n",
    "        #self.fc = nn.Linear(1*channels[4]*block.expansion, n_class)\n",
    "        self.conv5 = nn.Conv2d(channels[4], n_class, 1, 1, padding = 0)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', \n",
    "                                        nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        self.suffix = \"\"\n",
    "        \n",
    "        \n",
    "    def forward(self, x):                           # 1     30    30\n",
    "        x = self.layer0(x)                  # channels[0]   28    28\n",
    "        x = self.layer1(x)                  # channels[1]   28    28\n",
    "        x = self.maxpool(x)                 # channels[1]   14    14\n",
    "        x = self.layer2(x)                  # channels[2]   12    12\n",
    "        x = self.maxpool(x)                 # channels[2]    6     6\n",
    "        x = self.layer3(x)                  # channels[3]    6     6\n",
    "        x = self.maxpool(x)                 # channels[3]    3     3\n",
    "        x = self.layer4(x)                  # channels[4]    3     3\n",
    "        #x = self.maxpool(x)                # channels[4]    _     _\n",
    "        x = self.conv5(x)                   # n_class        1     1\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def _make_layer(self, block, planes, blocks, padding = 1):\n",
    "        # using padding to downsample. is padding == 0, size will reduce\n",
    "        # if padding == 1, size keeps same (given stride = 1, kernel_size = 3)\n",
    "        downsample = None\n",
    "        if padding != 1 or self.inplanes != planes * block.expansion:\n",
    "            # channel numbers or the width*height will change\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes*block.expansion, 3, 1, padding),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "                nn.Dropout2d(self.dropout_conv_p)\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, padding, self.dropout_conv_p,\n",
    "                            downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "afWZGqolF9jv"
   },
   "source": [
    "### Net6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wbObT_c6F8ZD"
   },
   "outputs": [],
   "source": [
    "class Net6(MyModule):\n",
    "    \"\"\"equal to Net5, a little faster\"\"\"\n",
    "    def __init__(self, \n",
    "                 channels,#= [32, 64, 128, 256, 512], \n",
    "                 p_fc = 0.0, \n",
    "                 p_conv = 0.0, \n",
    "                 n_class = 31,\n",
    "                 suffix = \"\"):\n",
    "        super(Net6, self).__init__(p_fc, p_conv, suffix)\n",
    "        self.suffix = \"\"\n",
    "        self.dropout_fc_p = p_fc\n",
    "        self.dropout_conv_p = p_conv\n",
    "        #self.dropout_conv = nn.Dropout2d(p = self.dropout_conv_p)\n",
    "        \n",
    "        self.layer1 = nn.Sequential( # keep size of each channel\n",
    "            nn.Conv2d(1, channels[0], 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm2d(channels[0]),\n",
    "            nn.Conv2d(channels[0], channels[1], 3, stride = 1, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm2d(channels[1]),                      # 30\n",
    "            nn.Dropout2d(p = self.dropout_conv_p)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential( # reduce size to: (old_size - 2) / 2\n",
    "            nn.Conv2d(channels[1], channels[2], 3),                  # 28\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm2d(channels[2]),\n",
    "            nn.MaxPool2d(2, 2),                     # 14\n",
    "            nn.Dropout2d(p = self.dropout_conv_p)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential( # reduce size to: (old_size - 2) / 2\n",
    "            nn.Conv2d(channels[2], channels[3], 3),                 # 12\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm2d(channels[3]),\n",
    "            nn.MaxPool2d(2, 2),                     #  6\n",
    "            nn.Dropout2d(p = self.dropout_conv_p)\n",
    "        )\n",
    "        self.layer4 = nn.Sequential( # reduce size to: (old_size - 2)\n",
    "            nn.Conv2d(channels[3], channels[4], 3, stride = 1, padding = 0),#  4\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.BatchNorm2d(channels[4]),\n",
    "            nn.MaxPool2d(2, 2),                      #  2\n",
    "            nn.Dropout2d(p = self.dropout_conv_p)\n",
    "        )\n",
    "     \n",
    "        #self.conv5 = nn.Conv2d(channels[4], n_class, 3)     #  1\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Dropout(self.dropout_fc_p),\n",
    "            nn.Linear(channels[4]*2*2, n_class)\n",
    "            #nn.Dropout(dr)\n",
    "            #nn.Linear(1000, n_class)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.layer1(x)\n",
    "        x += shortcut\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        #x = self.conv5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19gKYbGTtoLo"
   },
   "source": [
    "## 9. Preparation for Training Competitative Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lBgJvZt99_Rs"
   },
   "source": [
    "### merge dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7GEcsD4Ohzga"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    # Merge train_set(data augmented) and valid_set for data loader\n",
    "    # Here the train_set may be augmented\n",
    "    if use_augment_with_rotate:\n",
    "        train_set = np.concatenate((train_set, train_set_noised), axis = 0)\n",
    "    if use_augment_with_rotate or use_augment_with_rotate2:\n",
    "        train_set = np.concatenate((train_set, train_set_rotated), axis = 0)\n",
    "    if use_augment_with_fliplr:\n",
    "        train_set = np.concatenate((train_set, train_set_flipped), axis = 0)\n",
    "\n",
    "    train_valid_set = np.concatenate((train_set, valid_set), axis = 0)\n",
    "    debug(train_valid_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lwyKxIfz8gD9"
   },
   "outputs": [],
   "source": [
    "if SHOW_DEMO:\n",
    "    show_images([train_valid_set[:,:-1]], train_valid_set[:,-1], \n",
    "            class_label = 'squiggle', label_to_int = label_to_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjaNXELDtoLX"
   },
   "source": [
    "### method: dataloader\n",
    "\n",
    "Dataset is divided into several bathc-sized subset by the dataloader. The method supports both training/validating phase and test one. For training and validating, it receives a dataset with shape similar to [total_size, n_feature + 1], and yields a tuple `(X, y)` with both bath-sized, When used for testing, it receives dataset shaped [total_size, n_feature] and yields only `X` as test dataset has not labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11359,
     "status": "ok",
     "timestamp": 1543704415059,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "L1KGq_z0toLb",
    "outputId": "3f8b2542-5421-40a7-f51f-040f470fbd42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function dataloader at 0x7f14bbfd4840>\n"
     ]
    }
   ],
   "source": [
    "def dataloader(phase, data_source, \n",
    "               n_valid_sample = 1000, \n",
    "               batch_size = 128, \n",
    "               img_size = (30, 30)):\n",
    "    \"\"\"feed data to a training process with batch_size from a dataset\n",
    "    params\n",
    "        phase: decide whether the model is in 'train', 'val' or 'test'\n",
    "        data_source: dataset np.array (sample_size, n_features)\n",
    "        n_valid_sample: sample numbers for validating, int\n",
    "        batch_size: int\n",
    "        img_size: tuple, default (30, 30)\n",
    "    return Iterable()\n",
    "    \"\"\"\n",
    "    \n",
    "    class Iterable(object):\n",
    "        def __iter__(self):\n",
    "            \n",
    "            train_end = len(data_source) - n_valid_sample\n",
    "            # use image size to compute number of features, some data_source may\n",
    "            # not have the label(last column)\n",
    "            n_feature = img_size[0] * img_size[1]\n",
    "            \n",
    "            if phase == 'train':\n",
    "                data = data_source[ : train_end, :]\n",
    "                #np.random.shuffle(data) # shuffle in subset\n",
    "            elif phase == 'val':\n",
    "                data = data_source[train_end : , :]\n",
    "            else: # test dataset, no label column\n",
    "                data = data_source\n",
    "            \n",
    "            inputs = data[:,:n_feature]\n",
    "            if phase in ['train', 'val']: # have labels\n",
    "                labels = data[:, -1]\n",
    "            \n",
    "            sample_size = inputs.shape[0]\n",
    "            batches = int(np.ceil(sample_size / batch_size))\n",
    "            #debug(batches)\n",
    "            for j in range(batches):\n",
    "                #debug(\"{} in dataloader\".format(j))\n",
    "                b_start = j * batch_size\n",
    "                b_end = min(sample_size, (j + 1) * batch_size)\n",
    "                \n",
    "                batch_inputs = inputs[b_start:b_end, :]\n",
    "                batch_inputs = torch.from_numpy(batch_inputs)\n",
    "                batch_inputs = torch.unsqueeze(batch_inputs.view(\n",
    "                    b_end-b_start, img_size[0], img_size[1]), dim = 1)\n",
    "                \n",
    "                if phase in ['train', 'val']:\n",
    "                    batch_labels = labels[b_start:b_end]\n",
    "                    batch_labels = torch.from_numpy(batch_labels)\n",
    "                    batch_labels = batch_labels.type(torch.LongTensor)\n",
    "                    yield batch_inputs, batch_labels\n",
    "                else:\n",
    "                    yield batch_inputs\n",
    "    return Iterable()\n",
    "\n",
    "debug(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5CYn64ORdsw"
   },
   "source": [
    "### method: show_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9E2wmzLatoLo"
   },
   "outputs": [],
   "source": [
    "def show_learning_curve(losses, accuracies):\n",
    "    support = np.arange(len(losses['train']))\n",
    "    plt.figure(figsize = (14, 6))\n",
    "    plt.grid(True) # add a grid\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(support, losses['train'], 'g-', label = 'train')\n",
    "    plt.plot(support, losses['val'], 'b-', label = 'val')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('losses')\n",
    "    plt.title(\"Loss curves\")\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(support, accuracies['train'], 'g-', label = 'train' )\n",
    "    plt.plot(support, accuracies['val'], 'b-', label = 'val')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.title(\"Accuracy curves\")\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80-N0qLCRiAp"
   },
   "source": [
    "### method: load_model (from file)\n",
    "\n",
    "As training a model is a time-consuming work, sometimes we have to stop training but want the parameters saved so that we may continue our training later. Method `load_model` helps us finish this kind of work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dmkILaXdtoLt",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_model(model, \n",
    "               best_model_path = None, \n",
    "               auto_save_path = None,\n",
    "               best_model_first = False,\n",
    "               ext = \"\",\n",
    "              ):\n",
    "    \"\"\"load a pre-trained model from a file path\n",
    "    params\n",
    "        model: model, nn.Module\n",
    "        laod_best_model_first: Bool, if True, try first to load the model with\n",
    "            parameters has best performance on validating set\n",
    "        best_model_path: str\n",
    "        auto_save_path: str\n",
    "    return\n",
    "        model: with pre_trained parameters loaded, if failed to load either\n",
    "            parameters, return the model as it was.\n",
    "    \"\"\"\n",
    "\n",
    "    if best_model_path is None:\n",
    "        best_model_path = './models/best_model_' + model.get_name(ext)\n",
    "    if auto_save_path is None:\n",
    "        auto_save_path = './models/auto_save_' + model.get_name(ext)\n",
    "    \n",
    "    file_paths = [auto_save_path, best_model_path]\n",
    "    if best_model_first:\n",
    "        file_paths = [best_model_path, auto_save_path]\n",
    "\n",
    "    try:\n",
    "        debug(\"loading model from: '{}'... \".format(file_paths[0]), end=\" \")\n",
    "        model.load_state_dict(torch.load(file_paths[0]))\n",
    "        debug(\"successful.\")\n",
    "    except:\n",
    "        try:\n",
    "            debug(\"failure.\")\n",
    "            debug(\"loading model from: '{}'... \".format(file_paths[1]), \n",
    "                  end = \" \")\n",
    "            model.load_state_dict(torch.load(file_paths[1]))\n",
    "            debug(\"successful\")\n",
    "        except:\n",
    "            debug(\"failed.\\nusing new model parameters.\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wDRjhirFRS2u"
   },
   "source": [
    "### method: train_model (core)\n",
    "\n",
    "This is our key method where parameters of models are updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11690,
     "status": "ok",
     "timestamp": 1543704415432,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "j9gcF38BtoLh",
    "outputId": "a807c7de-c869-4fa9-d10b-3a1381d75d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function train_model at 0x7f14bbd36f28>\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, \n",
    "                train_valid_set,\n",
    "                criterion, \n",
    "                optimizer, \n",
    "                n_valid_sample,\n",
    "                scheduler = None, \n",
    "                auto_save_path = './models/auto_save.model',\n",
    "                best_model_path = './models/best_model_',\n",
    "                max_epochs = 20, \n",
    "                early_stopping = False, \n",
    "                epoch_patience = 5,\n",
    "                auto_load_best_after_train = True,\n",
    "                auto_save_interval = 1,\n",
    "                batch_size = 128,\n",
    "                show_live_progress = False\n",
    "                ):\n",
    "    \"\"\"train a model\n",
    "    params\n",
    "        model: to be trained, nn.Module\n",
    "        train_valid_set: merge of training and validating set\n",
    "        optimizer: optimizer\n",
    "        n_valid_sample: number of validating samples\n",
    "        scheduler: learning rate scheduler\n",
    "        auto_save_path: path where latest model will be auto saved\n",
    "        best_model_path: path where best model will be saved\n",
    "        max_epochs: max epochs, int\n",
    "        early_stopping: if trainint will stop earlier, according to accuracy on \n",
    "            validating dataset, Bool\n",
    "        epoch_patience: if accuracy on validate set decreases, we don't stop\n",
    "            at the epoch, we need to go extra epoch_patience number of epoch\n",
    "            to confirm this decreasing trends, int\n",
    "        auto_load_best_after_train: whether we load our best model paramters\n",
    "            after the training process is complete, Bool\n",
    "        auto_save_interval: the interval we auto save the parameters of the \n",
    "            model, int\n",
    "        batch_size: batch_size, int      \n",
    "    returns\n",
    "        model: trained, nn.Module\n",
    "        losses: losses during training and validating, \n",
    "            dict {\"train\", [double], 'val': [double]}\n",
    "        accuracies: accuracies during training and validating, \n",
    "            dict {\"train\", [double], 'val': [double]}\n",
    "        cache: a cache dict {'best_model_wts':best_model_wts,\n",
    "                             'best_acc_val':best_acc_val,\n",
    "                             'optimizer':optimizer,\n",
    "                             'model':model\n",
    "                            }\n",
    "    \"\"\"\n",
    "    if train_valid_set is None:    # no data to train\n",
    "        return model\n",
    "    \n",
    "    start = time.time()\n",
    "    dataset_sizes = {\n",
    "        'train': len(train_valid_set) - n_valid_sample,\n",
    "        'val': n_valid_sample\n",
    "    }\n",
    "    \n",
    "    dataloaders = {\n",
    "        'train':dataloader('train', \n",
    "                           data_source = train_valid_set, \n",
    "                           n_valid_sample = n_valid_sample,\n",
    "                           batch_size = batch_size, \n",
    "                           img_size = NEW_IMG_SIZE), \n",
    "        'val':dataloader('val', \n",
    "                         data_source = train_valid_set,\n",
    "                         n_valid_sample = n_valid_sample,\n",
    "                         batch_size = batch_size, \n",
    "                         img_size = NEW_IMG_SIZE)\n",
    "    }\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  # for best parameters\n",
    "    best_acc = {'train':0.0, 'val':0.0}                 # best accuracy\n",
    "    \n",
    "    losses = {'train': [], 'val':[]}\n",
    "    accuracies = {'train': [], 'val':[]}\n",
    "    patience_used = 0                                   # patience used\n",
    "    early_stopped = 0\n",
    "    debug(\"'s': auto saved, '*': best accuracy so far.\")\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        since = time.time()\n",
    "        \n",
    "        # s_before and s_after are for print trainig progress\n",
    "        s_before = '[Epoch{:>3d}/{} '.format(epoch, max_epochs - 1)\n",
    "        s_after = ']'\n",
    "        if show_live_progress:\n",
    "            print_progress(0, 0, s_before, s_after)\n",
    "        else:\n",
    "            print(s_before + s_after, end = \"\")\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            since_phase = time.time()\n",
    "            if phase == 'train':\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_size = 0\n",
    "            if show_live_progress:\n",
    "                print_progress(0, 0, s_before, s_after)\n",
    "            \n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, dim = 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # accumulate loss and correctly predicted sample number\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds.data == labels.data).item()\n",
    "                running_size += preds.size(0)\n",
    "                \n",
    "                if show_live_progress:\n",
    "                    time_elapsed = time.time() - since_phase\n",
    "                    print_progress(running_size/dataset_sizes[phase], \n",
    "                                   time_elapsed, \n",
    "                                   s_before, \n",
    "                                   s_after)\n",
    "                    already_show_estimated_time = True\n",
    "                # end batches loop\n",
    "                \n",
    "            # compute average loss and accuracy after a traing or validate phase\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            \n",
    "            # keep current loss and accuracy data\n",
    "            losses[phase].append(epoch_loss)\n",
    "            accuracies[phase].append(epoch_acc)\n",
    "            \n",
    "            time_elapsed = time.time() - since_phase\n",
    "            s_after += ' {} loss: {:.4f} acc: {:<7.2%} '.format(\n",
    "                phase, epoch_loss, epoch_acc)\n",
    "            \n",
    "\n",
    "            # check current parameters achieved best performance so far\n",
    "            # if YES, keep a reference to the best parameters.\n",
    "            if epoch_acc > best_acc[phase]:\n",
    "                best_acc[phase] = epoch_acc\n",
    "                if phase == 'val':\n",
    "                    s_after += \" *\"\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    patience_used = 0\n",
    "            else:\n",
    "                if phase == 'val': # avoid repeating check for both training\n",
    "                                   # and validating phase\n",
    "                    patience_used += 1 # \n",
    "                    if early_stopping and patience_used >= epoch_patience:\n",
    "                        # patience is used up\n",
    "                        early_stopped = epoch - epoch_patience\n",
    "                        \n",
    "            if show_live_progress:\n",
    "                print_progress(running_size/dataset_sizes[phase], \n",
    "                               time_elapsed, s_before, s_after)\n",
    "            else:\n",
    "                print(' {} loss: {:.4f} acc: {:<7.2%} '.format(\n",
    "                    phase, epoch_loss, epoch_acc), end = \" \")                        \n",
    "            #end phase loop\n",
    "    \n",
    "        time_elapsed = time.time() - since\n",
    "        if show_live_progress:\n",
    "            print_progress(1, time_elapsed, s_before, s_after)\n",
    "        else:\n",
    "            print(' {:.0f}m{:.0f}s'.format(\n",
    "                time_elapsed // 60, time_elapsed % 60), end = \"\")\n",
    "        # display some information\n",
    "        #if epoch_acc == best_acc['val']:\n",
    "        #    print(\"*\", end=\"\")\n",
    "        #else:\n",
    "        #    print(\"\", end=\"\")\n",
    "        if early_stopped or epoch == (max_epochs - 1) or \\\n",
    "           (epoch + 1) % auto_save_interval == 0:\n",
    "\n",
    "            torch.save(model.state_dict(), auto_save_path)\n",
    "            torch.save(best_model_wts, best_model_path)\n",
    "            print(\" s\") # saved\n",
    "        else:\n",
    "            print(\"\") # new line\n",
    "\n",
    "        if early_stopped > 0:\n",
    "            print(\"Early stop at epoch: {}\".format(early_stopped))\n",
    "            break\n",
    "        #end epoch loop \n",
    "    \n",
    "    \n",
    "    time_elapsed = time.time() - start\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        \n",
    "    print('Best val Acc on val:   {:.2%}'.format(best_acc['val']))\n",
    "    print('Best val Acc on train: {:.2%}'.format(best_acc['train']))\n",
    "    # load best model weights\n",
    "    \n",
    "    if auto_load_best_after_train:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        \n",
    "    train_info = {\n",
    "        \"best_acc_train\": best_acc[\"train\"],\n",
    "        \"best_acc_val\": best_acc[\"val\"],\n",
    "        \"time_elapsed\": time_elapsed,\n",
    "        \"early_stopped\": early_stopped,\n",
    "        \"epoch_passed\": epoch + 1\n",
    "    }\n",
    "    return model, losses, accuracies, train_info\n",
    "\n",
    "debug(train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tzUYPROdtoLx"
   },
   "source": [
    "## 10. Training Models\n",
    "\n",
    "From here, we start our training from the very begining: loading the original training data. All above variables / instances could be ignored, which means you can swith `SHOW_DEMO` to `False` and run all the code cells above only to load the necessary modules and the methods we just implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "klwDbf462nP_"
   },
   "outputs": [],
   "source": [
    "OLD_IMG_SIZE = (100, 100)\n",
    "NEW_IMG_SIZE = (30, 30)\n",
    "\n",
    "n_class = 31\n",
    "\n",
    "file_X_noise_reduced = './input/noise_reduced_train_X.npy'\n",
    "file_X_resized = './input/resized_and_noise_reduced_X.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21974,
     "status": "ok",
     "timestamp": 1543704425745,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "lMAhojX10f4a",
    "outputId": "e51cecb9-3af9-461b-acc4-dbff554f3401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading training labels from file: './input/train_labels.csv''\n",
      "size of image labels: 10000\n",
      "dict from int to label {0: 'shovel', 1: 'rifle', 2: 'scorpion', 3: 'apple', 4: 'spoon', 5: 'pineapple', 6: 'mouth', 7: 'skateboard', 8: 'rollerskates', 9: 'peanut', 10: 'rabbit', 11: 'sink', 12: 'sailboat', 13: 'nose', 14: 'skull', 15: 'pool', 16: 'pear', 17: 'pillow', 18: 'penguin', 19: 'nail', 20: 'pencil', 21: 'empty', 22: 'octagon', 23: 'moustache', 24: 'paintbrush', 25: 'panda', 26: 'parrot', 27: 'screwdriver', 28: 'squiggle', 29: 'rhinoceros', 30: 'mug'}\n",
      "dict from label to int {'shovel': 0, 'rifle': 1, 'scorpion': 2, 'apple': 3, 'spoon': 4, 'pineapple': 5, 'mouth': 6, 'skateboard': 7, 'rollerskates': 8, 'peanut': 9, 'rabbit': 10, 'sink': 11, 'sailboat': 12, 'nose': 13, 'skull': 14, 'pool': 15, 'pear': 16, 'pillow': 17, 'penguin': 18, 'nail': 19, 'pencil': 20, 'empty': 21, 'octagon': 22, 'moustache': 23, 'paintbrush': 24, 'panda': 25, 'parrot': 26, 'screwdriver': 27, 'squiggle': 28, 'rhinoceros': 29, 'mug': 30}\n",
      "train labels shape: (10000,)\n",
      "number of classes =  31\n",
      "loading training images from file: './input/train_images.npy'... \n",
      "successful\n",
      "shape of original training X(images):  (10000, 10000)\n",
      "loading 'noise reduced' from: './input/noise_reduced_train_X.npy'... successful.\n",
      "shape of 'noise reduced': (10000, 10000)\n",
      "loading 'X_resized' from: './input/resized_and_noise_reduced_X.npy'... successful.\n",
      "shape of 'X_resized': (10000, 900)\n"
     ]
    }
   ],
   "source": [
    "# load image labels data, build two mappings from label int to str, vice versa.\n",
    "image_labels, int_to_label, label_to_int = load_label_dataset()\n",
    "\n",
    "# load original image data (with noise, 10000 images with each has size 100*100\n",
    "X_original = load_image_dataset()\n",
    "\n",
    "# reduce noise\n",
    "X_noise_reduced = data_from_file(file_X_noise_reduced, \n",
    "                                 \"noise reduced\",\n",
    "                                 reduce_noise,\n",
    "                                 X_original,\n",
    "                                 img_size = OLD_IMG_SIZE)\n",
    "\n",
    "# crop and resize image to new size 30*30\n",
    "X_resized = data_from_file(file_X_resized, \n",
    "                           \"X_resized\",\n",
    "                           resize_dataset,\n",
    "                           X_noise_reduced,\n",
    "                           old_size = OLD_IMG_SIZE,\n",
    "                           new_size = NEW_IMG_SIZE                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJ6txeGaO35x"
   },
   "source": [
    "Switch on / off some control variables to perform diffrent data augmentation strategies. \n",
    "\n",
    "You are also free to change the hyper-parameters to see the different training procedure and output.\n",
    "\n",
    "Several models will be trained in order with each has a different randomized train / valid split. We may also keep a small size of dataset for our own test dataset to check the performance of the model specified by its validate sub-set. \n",
    "\n",
    "After all the models listed are trained, we may select a model that performes best on validate sub-set, or use `bagging` to make an ensemble prediction. \n",
    "\n",
    "See comments in the codes for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1635
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1398765,
     "status": "ok",
     "timestamp": 1543706172052,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "KkRo1tfOtoLy",
    "outputId": "81e62e4c-1a6c-47c6-daf8-dc5bcd2c95a2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global training configuration: \n",
      "('n_min_valid_set', 128)\n",
      "('n_test_set', 0)\n",
      "('shuffle_before_split', True)\n",
      "('use_norm', True)\n",
      "('use_bad_sample_removal', True)\n",
      "('use_rotate', False)\n",
      "('use_rotate2', True)\n",
      "('use_noising', False)\n",
      "('use_fliplr', True)\n",
      "('batch_size', 128)\n",
      "('trainer', 'YQ')\n",
      "('max_epochs', 30)\n",
      "('early_stopping', True)\n",
      "('epoch_patience', 5)\n",
      "('auto_load_best_after_train', True)\n",
      "('auto_save_interval', 5)\n",
      "('show_live_progress', True)\n",
      "normalizing data... complete.\n",
      "260 samples to be washed out\n",
      "washing out bad samples... complete. current data shape: (9740, 901)\n",
      "reserve 0 samples for testing\n",
      "\n",
      "================== model0: Net5 ================== \n",
      "training model: Net5_0.5_0.0__YQ_0, lr: 0.001, weight_decay:0\n",
      "shuffling train_valid_set before splitting... complete.\n",
      "split train_valid_set to 7740 / 2000 samples\n",
      "rotating image(special)...100.00% 0m19s complete\n",
      "dataset augmented with fliplr. \n",
      "train_set size:75108, valid_set size:2000\n",
      "'s': auto saved, '*': best accuracy so far.\n",
      "[Epoch  0/29 100.00% 1m48s] train loss: 1.2589 acc: 62.63%   val loss: 1.0083 acc: 71.75%   *\n",
      "[Epoch  1/29 100.00% 1m47s] train loss: 0.7558 acc: 76.90%   val loss: 0.8938 acc: 75.65%   *\n",
      "[Epoch  2/29 100.00% 1m46s] train loss: 0.5703 acc: 81.96%   val loss: 0.9359 acc: 77.20%   *\n",
      "[Epoch  3/29 100.00% 1m46s] train loss: 0.4516 acc: 85.63%   val loss: 0.9542 acc: 78.10%   *\n",
      "[Epoch  4/29 100.00% 1m46s] train loss: 0.3692 acc: 88.12%   val loss: 1.0310 acc: 77.55%   s\n",
      "[Epoch  5/29 100.00% 1m46s] train loss: 0.2204 acc: 92.86%   val loss: 0.8987 acc: 80.25%   *\n",
      "[Epoch  6/29 100.00% 1m45s] train loss: 0.1738 acc: 94.39%   val loss: 0.9124 acc: 80.85%   *\n",
      "[Epoch  7/29 100.00% 1m46s] train loss: 0.1528 acc: 95.01%   val loss: 0.9284 acc: 80.95%   *\n",
      "[Epoch  8/29 100.00% 1m46s] train loss: 0.1400 acc: 95.37%   val loss: 0.9496 acc: 80.75%  \n",
      "[Epoch  9/29 100.00% 1m46s] train loss: 0.1241 acc: 95.90%   val loss: 0.9658 acc: 80.35%   s\n",
      "[Epoch 10/29 100.00% 1m46s] train loss: 0.1089 acc: 96.35%   val loss: 0.9628 acc: 80.65%  \n",
      "[Epoch 11/29 100.00% 1m46s] train loss: 0.1043 acc: 96.57%   val loss: 0.9645 acc: 80.60%  \n",
      "[Epoch 12/29 100.00% 1m45s] train loss: 0.1012 acc: 96.71%   val loss: 0.9688 acc: 80.45%   s\n",
      "Early stop at epoch: 7\n",
      "Training complete in 22m 57s\n",
      "Best val Acc on val:   80.95%\n",
      "Best val Acc on train: 96.71%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz4AAAGCCAYAAAAhXBb/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f14bba02ba8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('n_min_valid_set', 128)\n",
      "('n_test_set', 0)\n",
      "('shuffle_before_split', True)\n",
      "('use_norm', True)\n",
      "('use_bad_sample_removal', True)\n",
      "('use_rotate', False)\n",
      "('use_rotate2', True)\n",
      "('use_noising', False)\n",
      "('use_fliplr', True)\n",
      "('batch_size', 128)\n",
      "('trainer', 'YQ')\n",
      "('max_epochs', 30)\n",
      "('early_stopping', True)\n",
      "('epoch_patience', 5)\n",
      "('auto_load_best_after_train', True)\n",
      "('auto_save_interval', 5)\n",
      "('show_live_progress', True)\n",
      "('best_acc_train', 0.9670873941524205)\n",
      "('best_acc_val', 0.8095)\n",
      "('time_elapsed', 1377.4413793087006)\n",
      "('early_stopped', 7)\n",
      "('epoch_passed', 13)\n"
     ]
    }
   ],
   "source": [
    "# you can put many models into a list so that they can be trained one after\n",
    "# another.\n",
    "# you can even assign different train / valid split for different models\n",
    "#\n",
    "'''\n",
    "models = [Net5(0.5), Net5(0.5), Net5(0.5), \n",
    "          Net5(0.45), Net5(0.45), Net5(0.45),\n",
    "          Net5(0.55), Net5(0.55), Net5(0.55)\n",
    "         ]\n",
    "'''\n",
    "# currently, we focused on single model `Net6`.\n",
    "# models = [Net6([32, 64, 128, 256, 256], p_fc = 0.2, p_conv = 0.2)]\n",
    "\n",
    "models = [Net5(p_fc = 0.5, channels = [32, 64, 128, 128, 128])]\n",
    "\n",
    "#models = [MyResNet(MyBlock, [2,1,0,0], [16, 64, 128, 256, 512], p = 0.0)]\n",
    "caches = []\n",
    "\n",
    "train_config = {\n",
    "    \"n_min_valid_set\": 128,\n",
    "    \"n_test_set\": 0,\n",
    "    \"shuffle_before_split\": True,\n",
    "    \"use_norm\": True, # use_normalization,\n",
    "    \"use_bad_sample_removal\": True, # use_bad_samples_removal,\n",
    "    \"use_rotate\": False, # use_augment_with_rotate,\n",
    "    \"use_rotate2\": True, # use_augment_with_rotate2,\n",
    "    \"use_noising\": False, #use_augment_with_noise,\n",
    "    \"use_fliplr\": True, #use_augment_with_fliplr,\n",
    "    \"batch_size\": 128,\n",
    "    \"trainer\": \"YQ\",\n",
    "    \"max_epochs\": 30,\n",
    "    \"early_stopping\": True,\n",
    "    \"epoch_patience\": 5,\n",
    "    \"auto_load_best_after_train\": True,\n",
    "    \"auto_save_interval\": 5,\n",
    "    \"show_live_progress\": True\n",
    "}\n",
    "\n",
    "debug(\"global training configuration: \")\n",
    "for item in train_config.items():\n",
    "    debug(item)\n",
    "\n",
    "\n",
    "# apply different train / val split for different models\n",
    "n_valid_sample = [2000] * len(models)\n",
    "# or set different number of validate set like:\n",
    "# n_valid_sample = [1500, 1000, 500, 2000, 1500... ] \n",
    "#[15000, 500, 500, 500, 500, 1000, 1000, 1000, 1000, 1000]\n",
    "# minimal smple number for validating\n",
    "\n",
    "# MAKE SURE USE THE SAME CONDITION ON TEST DATASET.\n",
    "if train_config[\"use_norm\"]:\n",
    "    X_resized = normalize_dataset(X_resized)\n",
    "    \n",
    "# all training samples are in total_set\n",
    "total_set = np.concatenate((X_resized, image_labels.reshape(-1, 1)), axis = 1)\n",
    "if train_config[\"use_bad_sample_removal\"]:\n",
    "    total_set = wash_samples(total_set)\n",
    "\n",
    "if train_config[\"shuffle_before_split\"]:\n",
    "    np.random.shuffle(total_set) # not need this?\n",
    "\n",
    "# split the total dataset to train_valid and test set.\n",
    "n_total_set = total_set.shape[0]\n",
    "test_start = n_total_set - train_config[\"n_test_set\"]\n",
    "\n",
    "# load pre-processed train data as a whole train_valid_test set.\n",
    "debug(\"reserve {} samples for testing\".format(train_config[\"n_test_set\"]))\n",
    "train_valid_set = total_set[:n_total_set - train_config[\"n_test_set\"],:]\n",
    "test_set = None\n",
    "if train_config[\"n_test_set\"] > 0:\n",
    "    test_set = total_set[n_total_set - train_config[\"n_test_set\"]:,:]\n",
    "    \n",
    "# training each model\n",
    "for i, model in enumerate(models):\n",
    "    \n",
    "    debug(\"\\n================== model{}: {} ================== \".format(\n",
    "        i, type(model).__name__))\n",
    " \n",
    "    hp_params = {\n",
    "        \"model_name\": type(model).__name__,\n",
    "        \"lr\": 1e-3,\n",
    "        \"weight_decay\": 0,\n",
    "        \"n_test_set\": train_config[\"n_test_set\"],\n",
    "        \"dropout_p\": model.dropout_p,\n",
    "        \"dropout_conv_p\": model.dropout_conv_p,\n",
    "    }\n",
    "    \n",
    "    # loss function\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = hp_params[\"lr\"], \n",
    "                           weight_decay = hp_params[\"weight_decay\"])\n",
    "    #scheduler = lr_scheduler.StepLR(optimizer, \n",
    "    #                                step_size = 5, \n",
    "    #                                gamma = 0.5)\n",
    "\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # Decay lr: first 10 epochs: 1e-3, last 10 epochs 1e-5, in middle: 1e-4\n",
    "    scheduler = lr_scheduler.MultiStepLR(\n",
    "        optimizer,  \n",
    "        milestones = [5,10,15,train_config[\"max_epochs\"]-5], \n",
    "        gamma = 0.1)\n",
    "    \n",
    "    suffix = \"_\" + train_config[\"trainer\"] + \"_\" + str(i)\n",
    "    debug('training model: {}, lr: {}, weight_decay:{}'.format(\n",
    "        model.get_name(suffix), \n",
    "        optimizer.param_groups[0]['lr'], \n",
    "        optimizer.param_groups[0]['weight_decay']))\n",
    "    \n",
    "    if train_config[\"shuffle_before_split\"]:\n",
    "        debug(\"shuffling train_valid_set before splitting... \", end = \"\")\n",
    "        np.random.shuffle(train_valid_set)\n",
    "        debug(\"complete.\")\n",
    "    \n",
    "    # reload train_valid_set from total_set\n",
    "    train_valid_set = total_set[:n_total_set - train_config[\"n_test_set\"],:]\n",
    "    \n",
    "    # split dataset to train / valid sets\n",
    "    n_train_valid_set = train_valid_set.shape[0]\n",
    "    cur_valid_sample = max(n_valid_sample[i], train_config[\"n_min_valid_set\"])\n",
    "    hp_params[\"n_valid_set\"] = cur_valid_sample\n",
    "\n",
    "    train_set = train_valid_set[0: -cur_valid_sample,:]\n",
    "    valid_set = train_valid_set[-cur_valid_sample: ,:]\n",
    " \n",
    "    debug(\"split train_valid_set to {} / {} samples\".format(\n",
    "        n_train_valid_set - cur_valid_sample, cur_valid_sample))\n",
    "    \n",
    "    #view_data_distribution(valid_set[:,-1], \"valid_set\", int_to_label)\n",
    "\n",
    "    \n",
    "    # data augmentation on training set\n",
    "    augmented_train_set_list = []\n",
    "    if train_config[\"use_rotate\"]:\n",
    "        augmented_train_set_list.append(augment_with_rotate(train_set))\n",
    "    if train_config[\"use_rotate2\"]:\n",
    "        augmented_train_set_list.append(augment_with_rotate2(train_set))\n",
    "    if train_config[\"use_noising\"]:\n",
    "        for i in range(4):\n",
    "            augmented_train_set_list.append(augment_with_noise(train_set))\n",
    "    if train_config[\"use_fliplr\"]:\n",
    "        augmented_train_set_list.append(augment_with_fliplr(train_set))\n",
    "        \n",
    "    for augmented_train_set in augmented_train_set_list:\n",
    "        train_set = np.concatenate((train_set, augmented_train_set), axis = 0)\n",
    "\n",
    "    n_train_set = train_set.shape[0]\n",
    "    hp_params[\"n_train_set\"] = n_train_set\n",
    "    \n",
    "    np.random.shuffle(train_set)\n",
    "    #view_data_distribution(train_set[:,-1], \"train_set\", int_to_label)\n",
    "    \n",
    "    # build new train_valid_set by merging train set with valid set\n",
    "    train_valid_set = np.concatenate((train_set, valid_set), axis = 0)\n",
    "    \n",
    "    # set model save path\n",
    "    best_model_path = './models/best_model_' + model.get_name(suffix)\n",
    "    auto_save_path = './models/auto_save_' + model.get_name(suffix)\n",
    "    \n",
    "    # pre-trained model from file\n",
    "    # careful: this may cause model saw samples in validating set.\n",
    "    # must NOT use pre-trained model if whole train / valid set si shuffled\n",
    "    #model = load_model(model, best_model_path, auto_save_path)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    debug(\"train_set size:{}, valid_set size:{}\".format(\n",
    "        hp_params[\"n_train_set\"], hp_params[\"n_valid_set\"]))\n",
    "    \n",
    "    # perform training process\n",
    "    model, loss, accuracies, train_result = train_model(\n",
    "        model, \n",
    "        train_valid_set,\n",
    "        criterion, \n",
    "        optimizer, \n",
    "        n_valid_sample = cur_valid_sample,\n",
    "        scheduler = scheduler, \n",
    "        max_epochs = train_config[\"max_epochs\"],\n",
    "        early_stopping = train_config[\"early_stopping\"],\n",
    "        epoch_patience = train_config[\"epoch_patience\"],\n",
    "        auto_load_best_after_train = train_config[\"auto_load_best_after_train\"],\n",
    "        auto_save_path = auto_save_path, \n",
    "        best_model_path = best_model_path,\n",
    "        auto_save_interval = train_config[\"auto_save_interval\"],\n",
    "        batch_size = train_config[\"batch_size\"],\n",
    "        show_live_progress = train_config[\"show_live_progress\"]\n",
    "    )\n",
    "    caches.append(train_result)\n",
    "    \n",
    "    show_learning_curve(loss, accuracies)  \n",
    "    for item in train_config.items():\n",
    "        debug(item)\n",
    "    for item in train_result.items():\n",
    "        debug(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1543706278378,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "y3uOsUowSrf9",
    "outputId": "071d222c-a08b-4ca4-b782-e25ef3d0495f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('best_acc_train', 0.9670873941524205), ('best_acc_val', 0.8095), ('time_elapsed', 1377.4413793087006), ('early_stopped', 7), ('epoch_passed', 13)])\n"
     ]
    }
   ],
   "source": [
    "# view model architecture and their best accuracies on validate dataset.\n",
    "for cache in caches:\n",
    "    print(cache.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CTAoGBKuyYGO"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0tXlWAltoL1"
   },
   "source": [
    "## 11. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 830,
     "status": "ok",
     "timestamp": 1543706281702,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "P74kTNlE0cZ8",
    "outputId": "5526b351-da3c-46b5-c87a-d33b26a96362"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrainers = [\"YQ\"]\\nmodels = [Net5(), Net5(0.45), Net5(0.55), \\n          Net5(), Net5(0.45), Net5(0.55), \\n          Net5(), Net5(0.45), Net5(0.55),\\n         ] * len(trainers)\\n\\n\\nfor i, model in enumerate(models):\\n    for trainer in trainers:\\n        suffix = \"_\" + trainer + \"_\" + str(i)\\n        best_model_path = \\'./models/best_model_\\' + model.get_name(suffix)\\n        auto_save_path = \\'./models/auto_save_\\' + model.get_name(suffix)\\n        model = load_model(model, \\n                           best_model_path = best_model_path,\\n                           auto_save_path = auto_save_path,\\n                           best_model_first = True,\\n                           ext = suffix)\\n'"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment and run this cell if you load models from model files\n",
    "'''\n",
    "trainers = [\"YQ\"]\n",
    "models = [Net5(), Net5(0.45), Net5(0.55), \n",
    "          Net5(), Net5(0.45), Net5(0.55), \n",
    "          Net5(), Net5(0.45), Net5(0.55),\n",
    "         ] * len(trainers)\n",
    "\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    for trainer in trainers:\n",
    "        suffix = \"_\" + trainer + \"_\" + str(i)\n",
    "        best_model_path = './models/best_model_' + model.get_name(suffix)\n",
    "        auto_save_path = './models/auto_save_' + model.get_name(suffix)\n",
    "        model = load_model(model, \n",
    "                           best_model_path = best_model_path,\n",
    "                           auto_save_path = auto_save_path,\n",
    "                           best_model_first = True,\n",
    "                           ext = suffix)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8EbKVJgDtoL2"
   },
   "outputs": [],
   "source": [
    "def predict(model, test_X):\n",
    "    \"\"\"predict using a model on a dataset(test_X)\n",
    "    params\n",
    "        model: nn.Module\n",
    "        test_X: dataset, narray (n_size, n_feature)\n",
    "    returns\n",
    "        predict_labels: prediction, list [n_size]\n",
    "    \"\"\"\n",
    "    since = time.time()\n",
    "    model.to(device)\n",
    "    try:\n",
    "        model.eval()   # Set model to evaluate mode\n",
    "    except:\n",
    "        pass\n",
    "    predict_labels = []\n",
    "    # Iterate over data.\n",
    "    i = 0\n",
    "    total = len(test_X)\n",
    "    test_data = dataloader(phase = 'test', data_source = test_X)\n",
    "    for batch_inputs in test_data:\n",
    "        #debug(batch_inputs.shape)\n",
    "        batch_inputs = batch_inputs.to(device)\n",
    "        with torch.set_grad_enabled(False):\n",
    "            batch_outputs = model(batch_inputs)\n",
    "            batch_predict = torch.argmax(batch_outputs, dim = 1)\n",
    "            batch_predict = batch_predict.cpu().numpy().tolist()\n",
    "            predict_labels.extend(batch_predict)\n",
    "            i += batch_inputs.size(0)\n",
    "            print_progress(i / total)\n",
    "    return predict_labels\n",
    "\n",
    "\n",
    "def predicts_from_models(models, test_X):\n",
    "    \"\"\"return an ensemble predicts of all models on test_X\n",
    "    params\n",
    "        models: list [model]\n",
    "        test_X: test set, narray (n_samples, n_features)\n",
    "    returns\n",
    "        predicts: narray (n_samples, len(models))\n",
    "    \"\"\"\n",
    "    n_models = len(models)\n",
    "    n_samples = test_X.shape[0]\n",
    "    predicts = np.zeros((n_samples, n_models))\n",
    "    for i, model in enumerate(models):\n",
    "        predicts[:,i] = np.array(predict(model, test_X)).reshape(-1, )\n",
    "    \n",
    "    return predicts\n",
    "\n",
    "\n",
    "def bagging(all_predicts):\n",
    "    \"\"\"giving a bagging predict based on all_predicts\n",
    "    params\n",
    "        all_predicts: narray (n_samples, n_models)\n",
    "    returns\n",
    "        voted_predict: list, \n",
    "    \"\"\"\n",
    "    n_samples, n_models = all_predicts.shape\n",
    "    voted_predict = [None] * n_samples\n",
    "    for i in range(n_samples):\n",
    "        voted_predict[i] = np.argmax(np.bincount(all_predicts[i].astype(int)))\n",
    "    return voted_predict\n",
    "\n",
    "\n",
    "def bagging_from_models(models, test_X):\n",
    "    \"\"\"ensemble predic using bagging\n",
    "    params\n",
    "        models: list of model of nn.Module, [nn.Module]\n",
    "        test_X: dataset, narray (n_size, n_feature)\n",
    "    returns\n",
    "        voted_predicted: prediction, list [n_size]\n",
    "        predicts: predicts by all models, narray [n_size, len(models)]\n",
    "    \"\"\"\n",
    "    predicts = predicts_from_models(models, test_X)\n",
    "    voted_predict = bagging(predicts)   \n",
    "    return voted_predict, predicts\n",
    "    \n",
    "\n",
    "def analysis_predict(true_labels, predict_labels, n_class = 31):\n",
    "    \"\"\"give an averate accuracy, show accuracies in each class.\n",
    "    you can switch the values of the two parameters to see difference output\n",
    "    params\n",
    "        true_labels: narray (-1, ) or (-1, 1)\n",
    "        predict_labels: narray (-1, ) or (-1, 1)\n",
    "    returns\n",
    "        average_accuracy\n",
    "    \"\"\"\n",
    "    debug(\"Size: {}\".format(len(true_labels)))\n",
    "    class_correct = [0 for _ in range(n_class)]\n",
    "    class_total = [0 for _ in range(n_class)]\n",
    "    for i in range(len(predict_labels)):\n",
    "        label_int = int(true_labels[i])\n",
    "        class_correct[label_int] += (predict_labels[i] == label_int)\n",
    "        class_total[label_int] += 1\n",
    "        \n",
    "    average_accuracy = sum(class_correct) / sum(class_total)\n",
    "    print(\"Average accuracy: {:.2%}\".format(average_accuracy))\n",
    "\n",
    "    print(\"-\"*32)\n",
    "    for i in range(n_class):\n",
    "        print(\"Accuracy of {:>12s}: {:.2%}\".format(\n",
    "            int_to_label[i], class_correct[i] / class_total[i]))\n",
    "\n",
    "    return average_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NHIxAigTtoL4"
   },
   "source": [
    "**Test on reserved test set and show accuracy**\n",
    "\n",
    "If you don't reserve test samples from the training dataset, skip the following cells in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxfmLEFPtoL4"
   },
   "outputs": [],
   "source": [
    "# using test_set\n",
    "# test_set = total_set[n_test_set:,:]\n",
    "if test_set is not None:\n",
    "    test_inputs = test_set[:,:-1]\n",
    "    test_labels = test_set[:, -1]\n",
    "    debug(test_set.shape)\n",
    "    test_predict_bagging, test_predicts_all = bagging_from_models(models, test_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N_LUO0V1Ex5P"
   },
   "outputs": [],
   "source": [
    "#debug(test_predict_bagging[0:100])\n",
    "#debug(test_predicts_all[0:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cIljq-gitoL9",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if test_set is not None:\n",
    "    average_accuracy = analysis_predict(test_labels, test_predict_bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3PUUEyT7oX5"
   },
   "outputs": [],
   "source": [
    "if test_set is not None:\n",
    "    view_data_distribution(test_predict_bagging, \"test_predict\", int_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MReKllSroHJQ"
   },
   "outputs": [],
   "source": [
    "if test_set is not None:\n",
    "    show_images([test_set[:,:-1]], test_predict_bagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ISQylsYzHPLw"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2IhBYt4-T5q"
   },
   "source": [
    "view mis-classified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rgRSKciq-SPE"
   },
   "outputs": [],
   "source": [
    "# view indices of misclassified images\n",
    "if test_set is not None:\n",
    "    is_correct = (np.array(test_predict_bagging) == test_labels)\n",
    "    mis_indices = np.argwhere(is_correct == False).reshape(-1,)\n",
    "    print(mis_indices)\n",
    "    debug(len(mis_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bt8VRSbH_jSU"
   },
   "outputs": [],
   "source": [
    "if test_set is not None:\n",
    "    \n",
    "    # show mis-classified images and predictions\n",
    "    # this may takes some time to show all the images wrong classified.\n",
    "    for id in mis_indices:\n",
    "        debug(\"predicted:\", int_to_label[test_predict_bagging[id]])\n",
    "        show_images(test_set[:,0:-1], test_set[:,-1], id)\n",
    "        print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "69Bf4HmZtoL_"
   },
   "source": [
    "Same result by confusion_matrix from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fCXoi6qtoMJ"
   },
   "outputs": [],
   "source": [
    "# show confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "if test_set is not None:\n",
    "    cm = confusion_matrix(test_labels, test_predict_bagging)\n",
    "    plt.imshow(cm)\n",
    "\n",
    "#class_corrects = np.max(cm, axis = 1)\n",
    "#class_totals = np.sum(cm, axis = 1)\n",
    "#average_accuracy = np.sum(class_corrects) / np.sum(class_totals)\n",
    "#accuracy = class_corrects / class_totals\n",
    "\n",
    "#print(\"Average accuracy(valid) : {:.2%}\".format(average_accuracy))\n",
    "#print(\"-\"*32)\n",
    "#for i in range(n_class):\n",
    "#    print(\"Accuracy of {:>12s}: {:.2%}\".format(\n",
    "#        int_to_label[i], accuracy[i] ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mDN9ZtxRtoML"
   },
   "source": [
    "## 12. Testing Models on Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hgd-SztoQsf4"
   },
   "source": [
    "### loading test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x-TgAG1_toMM"
   },
   "outputs": [],
   "source": [
    "test_images = None\n",
    "test_X_noise_reduced = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RhuqXpb6toMO",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_image_data = np.load('./input/test_images.npy', encoding = 'latin1')\n",
    "test_size, test_features = test_image_data.shape[0], len(test_image_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1333,
     "status": "ok",
     "timestamp": 1543706450216,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "FwbE6-SktoMR",
    "outputId": "b6cb078e-cbb8-40cd-a791-7cc307b838e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 10000)\n"
     ]
    }
   ],
   "source": [
    "test_images = np.vstack(test_image_data[:,1]) # test_size, 100, 100\n",
    "debug(test_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5031,
     "status": "ok",
     "timestamp": 1543706456910,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "2s0t9PmotoMW",
    "outputId": "9671d606-b940-4ab1-cda6-1ffccba86e33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 'test_noise_reduced' from: './input/noise_reduced_test_X.npy'... successful.\n",
      "shape of 'test_noise_reduced': (10000, 10000)\n",
      "(10000, 10000)\n"
     ]
    }
   ],
   "source": [
    "file_test_X_noise_reduced = './input/noise_reduced_test_X.npy'\n",
    "test_X_noise_reduced = data_from_file(\n",
    "    file_test_X_noise_reduced,\n",
    "    \"test_noise_reduced\",\n",
    "    reduce_noise,\n",
    "    test_images)\n",
    "   \n",
    "debug(test_X_noise_reduced.shape)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL6a67kcQzX6"
   },
   "source": [
    "### performing the same data pre-process procedure as train / val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 38027,
     "status": "ok",
     "timestamp": 1543706496770,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "85R_H-nd7i5D",
    "outputId": "d9f12e89-4245-46b9-8dba-ed6d8809ddaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resizing images... \n",
      "progress:100.00% 0m27snormalizing data... complete.\n"
     ]
    }
   ],
   "source": [
    "# make sure that both resized_test_X and resized_train_X are generated by\n",
    "# the same data pre-processes.\n",
    "OLD_IMG_SIZE = (100, 100)\n",
    "NEW_IMG_SIZE = (30, 30)\n",
    "file_test_X_resized = './input/resized_test_X.npy'\n",
    "\n",
    "test_X_resized = resize_dataset(\n",
    "    test_X_noise_reduced,\n",
    "    old_size = OLD_IMG_SIZE,\n",
    "    new_size = NEW_IMG_SIZE\n",
    ")\n",
    "use_normalization = True\n",
    "\n",
    "# MAKE SURE USE THE SAME CONDITION ON TRAINING DATASET.\n",
    "if use_normalization:\n",
    "    test_X_resized = normalize_dataset(test_X_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 890,
     "status": "ok",
     "timestamp": 1543706522596,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "-tFKU4lOtoMa",
    "outputId": "a13cfa40-bdd1-4eeb-b152-423f4c6b2309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:2592\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApwAAADFCAYAAAD5VSKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nVThFREREJFCqcIqIiIhIoP4/SGr+B/AllsoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f14bbbece80>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# before make prediction, let's see some test images\n",
    "# re-run this cell to show different images on test dataset.\n",
    "show_images([test_X_resized, test_X_noise_reduced, test_images], is_test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ml6T_6S9e7Y"
   },
   "outputs": [],
   "source": [
    "# release memory, these instances are only used for display images.\n",
    "test_images = None # release memory\n",
    "test_X_noise_reduced = None # release memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RNtx4IDCQ_U4"
   },
   "source": [
    "### loading models from file if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 265,
     "status": "ok",
     "timestamp": 1543706526613,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "pZ7sNOJrtoMe",
    "outputId": "7af611c6-1edb-43de-9d56-b3f459f986f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrainers = [\"YQ\"]\\nmodels = [Net5(), Net5(), Net5(), Net5(), Net5(), Net5(), Net5(), Net5(), \\n          Net5(0.45), Net5(0.45), Net5(0.45), Net5(0.45), Net5(0.45), Net5(0.45),\\n          Net5(0.55), Net5(0.55), Net5(0.55), Net5(0.55), Net5(0.55), Net5(0.55)\\n         ]\\n\\n\\nfor i, model in enumerate(models):\\n    for trainer in trainers:\\n        suffix = \"_\" + trainer + \"_\" + str(i)\\n        best_model_path = \\'./models/best_model_\\' + model.get_name(suffix)\\n        auto_save_path = \\'./models/auto_save_\\' + model.get_name(suffix)\\n        model = load_model(model, \\n                           best_model_path = best_model_path,\\n                           auto_save_path = auto_save_path,\\n                           best_model_first = True,\\n                           ext = suffix)\\n        model.eval()\\n'"
      ]
     },
     "execution_count": 89,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment the following codes to load model from files\n",
    "# make reasonable modification to these codes\n",
    "'''\n",
    "trainers = [\"YQ\"]\n",
    "models = [Net5(), Net5(), Net5(), Net5(), Net5(), Net5(), Net5(), Net5(), \n",
    "          Net5(0.45), Net5(0.45), Net5(0.45), Net5(0.45), Net5(0.45), Net5(0.45),\n",
    "          Net5(0.55), Net5(0.55), Net5(0.55), Net5(0.55), Net5(0.55), Net5(0.55)\n",
    "         ]\n",
    "\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    for trainer in trainers:\n",
    "        suffix = \"_\" + trainer + \"_\" + str(i)\n",
    "        best_model_path = './models/best_model_' + model.get_name(suffix)\n",
    "        auto_save_path = './models/auto_save_' + model.get_name(suffix)\n",
    "        model = load_model(model, \n",
    "                           best_model_path = best_model_path,\n",
    "                           auto_save_path = auto_save_path,\n",
    "                           best_model_first = True,\n",
    "                           ext = suffix)\n",
    "        model.eval()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R8BULng7RItF"
   },
   "source": [
    "### predicting and bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4558,
     "status": "ok",
     "timestamp": 1543706533730,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "Q5g0lxDHtoMg",
    "outputId": "dbba7eb7-251c-4210-ed3b-e88363152263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress:100.00%"
     ]
    }
   ],
   "source": [
    "# get all models' prediction on test dataset and their bagging prediction\n",
    "predict_bagging, all_predicts = bagging_from_models(models, test_X_resized)\n",
    "current_model_name = \"bagging\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1543706536843,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "54g1DsvNtoMj",
    "outputId": "a6c19e12-356b-4d1e-a4e6-f197ee8623c5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'shovel', 1: 'rifle', 2: 'scorpion', 3: 'apple', 4: 'spoon', 5: 'pineapple', 6: 'mouth', 7: 'skateboard', 8: 'rollerskates', 9: 'peanut', 10: 'rabbit', 11: 'sink', 12: 'sailboat', 13: 'nose', 14: 'skull', 15: 'pool', 16: 'pear', 17: 'pillow', 18: 'penguin', 19: 'nail', 20: 'pencil', 21: 'empty', 22: 'octagon', 23: 'moustache', 24: 'paintbrush', 25: 'panda', 26: 'parrot', 27: 'screwdriver', 28: 'squiggle', 29: 'rhinoceros', 30: 'mug'}\n",
      "[12, 28, 2, 21, 19, 3, 16, 30, 1, 23, 13, 6, 21, 10, 20, 0, 9, 1, 28, 21, 5, 12, 15, 23, 3, 13, 12, 15, 0, 30, 10, 17, 21, 24, 0, 12, 8, 2, 17, 12, 19, 2, 19, 15, 29, 25, 16, 13, 6, 20, 28, 1, 0, 23, 15, 19, 3, 4, 10, 22, 26, 13, 20, 15, 2, 18, 28, 21, 2, 22, 16, 1, 18, 9, 18, 19, 21, 23, 28, 29, 18, 12, 8, 5, 17, 19, 11, 12, 22, 21, 5, 2, 12, 11, 5, 14, 17, 0, 26, 6]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 91,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view some prediction data\n",
    "print(int_to_label)\n",
    "print(predict_bagging[100:200])\n",
    "len(predict_bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1543706539110,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "Rb348ajxtoMq",
    "outputId": "38a21778-6199-43fe-9b34-e1b928dcb43f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image id:7072, class_id:23, class_label:moustache\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMMAAADDCAYAAAA/f6WqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/Q+SApxb9XOqLwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f14b792d0f0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show an image and corresponding bagging prediction\n",
    "show_images([test_X_resized, test_X_noise_reduced, test_images], predict_bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 268,
     "status": "ok",
     "timestamp": 1543706541766,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "iVCLuG9iLJ6N",
    "outputId": "a760ca15-fde6-4950-e26f-66519f22aed6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of:  test predict\n",
      "classes:31 total: 10000 (100%) \n",
      "--------------------------------\n",
      "          shovel: 323 (3.23%)\n",
      "           rifle: 422 (4.22%)\n",
      "        scorpion: 340 (3.40%)\n",
      "           apple: 352 (3.52%)\n",
      "           spoon: 160 (1.60%)\n",
      "       pineapple: 492 (4.92%)\n",
      "           mouth: 569 (5.69%)\n",
      "      skateboard: 406 (4.06%)\n",
      "    rollerskates: 280 (2.80%)\n",
      "          peanut: 259 (2.59%)\n",
      "          rabbit: 345 (3.45%)\n",
      "            sink: 259 (2.59%)\n",
      "        sailboat: 396 (3.96%)\n",
      "            nose: 193 (1.93%)\n",
      "           skull: 291 (2.91%)\n",
      "            pool: 435 (4.35%)\n",
      "            pear: 176 (1.76%)\n",
      "          pillow: 261 (2.61%)\n",
      "         penguin: 324 (3.24%)\n",
      "            nail: 342 (3.42%)\n",
      "          pencil: 313 (3.13%)\n",
      "           empty: 370 (3.70%)\n",
      "         octagon: 450 (4.50%)\n",
      "       moustache: 437 (4.37%)\n",
      "      paintbrush: 275 (2.75%)\n",
      "           panda: 180 (1.80%)\n",
      "          parrot: 342 (3.42%)\n",
      "     screwdriver: 198 (1.98%)\n",
      "        squiggle: 255 (2.55%)\n",
      "      rhinoceros: 179 (1.79%)\n",
      "             mug: 376 (3.76%)\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# view predicted label distribution\n",
    "# check if they are similar to the train or valid dataset.\n",
    "view_data_distribution(predict_bagging, \"test predict\", int_to_label)\n",
    "#  special_labels = ['peanut', 'spoon', 'shovel', 'screwdriver', 'squiggle',\n",
    "#                      'pencil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F3fN4OX6MAG4"
   },
   "outputs": [],
   "source": [
    "# you can also check a single model's prediction and predicted label\n",
    "# distribution, uncomment next two lines\n",
    "\n",
    "# single_predict = predict(models[0], test_X_resized)\n",
    "# view_data_distribution(predict_bagging, \"test predict\", int_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gAw-khA-RN2H"
   },
   "source": [
    "### generate / load csv predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n0JY5D-xtoMt"
   },
   "outputs": [],
   "source": [
    "# let's export our predictions to the csv file for submitting\n",
    "# build a method for one prediction.\n",
    "import csv\n",
    "import glob\n",
    "\n",
    "def predict_to_csv(predicts, file_path):\n",
    "    with open(file_path, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter = ',')\n",
    "        writer.writerow((\"Id\", \"Category\"))\n",
    "        for i, label in enumerate(predicts):\n",
    "            writer.writerow((i, int_to_label[predicts[i]]))\n",
    "    debug(\"save predict csv files of model: {} to: {}\".format(\n",
    "            current_model_name, file_path))\n",
    "    \n",
    "    \n",
    "def predict_from_csv(csv_path, label_to_int):\n",
    "    csv_data = np.genfromtxt(csv_path, names=True, delimiter=',',\n",
    "                        dtype=[('Id', 'i8'), ('Category', 'S16')])\n",
    "    debug(\"importing prediction from:{}... \".format(csv_path), end = \"\")\n",
    "    predict = np.zeros((csv_data.shape[0], ), dtype = np.long)\n",
    "    for i, predict_label in csv_data:\n",
    "        predict_label = predict_label.decode(\"utf-8\")\n",
    "        predict[i] = label_to_int[predict_label]\n",
    "    debug(\"complete.\")\n",
    "    return predict\n",
    "\n",
    "\n",
    "def predicts_from_csvs(file_path, n_size, label_to_int):\n",
    "    #file_path = \"./candidates/top/*.csv\"\n",
    "    candidates = glob.glob(file_path)\n",
    "    n_files = len(candidates)\n",
    "    if n_files == 0:\n",
    "        debug(\"No submission files found.\")\n",
    "        return None\n",
    "    predicts = np.zeros((n_size, n_files))\n",
    "    voted_predict = [None] * n_size\n",
    "\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        predicts[:, i] = predict_from_csv(candidate, label_to_int)\n",
    "    return predicts\n",
    "\n",
    "\n",
    "def bagging_from_csvs(file_path, n_size, label_to_int):\n",
    "    all_predicts = predicts_from_csvs(file_path, n_size, label_to_int)\n",
    "    if all_predicts is None:\n",
    "        return None, None\n",
    "    voted_predict = bagging(all_predicts)\n",
    "    return voted_predict, all_predicts    \n",
    "\n",
    "# lazy_bagging: collect csv files to make bagging predcition, efficient way\n",
    "# don't need to load models and generate predictions from them.\n",
    "lazy_bagging = bagging_from_csvs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2462,
     "status": "ok",
     "timestamp": 1543706557311,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "-QLuKSuuFP2d",
    "outputId": "6cdd3851-a64b-4ed2-916e-5d72fd5aa429"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing prediction from:./candidates/top/lazy_bagging_result_great_than_83.csv... complete.\n",
      "importing prediction from:./candidates/top/lazy_bagging_kaggle_83.40.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_83.00.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_83.40.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_83.00.1.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_83.33.1.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_80.44.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_82.58.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_82.80.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_83.58.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_84.15.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_83.87.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_82.75.csv... complete.\n",
      "importing prediction from:./candidates/top/lazy_bagging_result_great_than_80.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_83.25.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_83.53.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_84.93.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_80.33.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_82.29.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_83.27.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_82.20.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_84.43.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_82.73.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_83.11.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_82.93.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_79.40.csv... complete.\n",
      "importing prediction from:./candidates/top/lazy_bagging_result_great_than_84.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_78.80.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_82.60.csv... complete.\n",
      "importing prediction from:./candidates/top/submission_83.33.csv... complete.\n"
     ]
    }
   ],
   "source": [
    "voted_predict, all_predicts = bagging_from_csvs(\"./candidates/top/*csv\", 10000, label_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1543706563170,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "kwFzwvr0He8N",
    "outputId": "4bd10d9f-49e7-432e-ccd2-7cc78461f6a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of:  bagging from csvs\n",
      "classes:31 total: 10000 (100%) \n",
      "--------------------------------\n",
      "          shovel: 293 (2.93%)\n",
      "           rifle: 461 (4.61%)\n",
      "        scorpion: 365 (3.65%)\n",
      "           apple: 358 (3.58%)\n",
      "           spoon: 181 (1.81%)\n",
      "       pineapple: 521 (5.21%)\n",
      "           mouth: 565 (5.65%)\n",
      "      skateboard: 416 (4.16%)\n",
      "    rollerskates: 276 (2.76%)\n",
      "          peanut: 213 (2.13%)\n",
      "          rabbit: 370 (3.70%)\n",
      "            sink: 262 (2.62%)\n",
      "        sailboat: 415 (4.15%)\n",
      "            nose: 191 (1.91%)\n",
      "           skull: 292 (2.92%)\n",
      "            pool: 473 (4.73%)\n",
      "            pear: 175 (1.75%)\n",
      "          pillow: 248 (2.48%)\n",
      "         penguin: 368 (3.68%)\n",
      "            nail: 362 (3.62%)\n",
      "          pencil: 289 (2.89%)\n",
      "           empty: 372 (3.72%)\n",
      "         octagon: 456 (4.56%)\n",
      "       moustache: 446 (4.46%)\n",
      "      paintbrush: 312 (3.12%)\n",
      "           panda: 189 (1.89%)\n",
      "          parrot: 161 (1.61%)\n",
      "     screwdriver: 158 (1.58%)\n",
      "        squiggle: 214 (2.14%)\n",
      "      rhinoceros: 214 (2.14%)\n",
      "             mug: 384 (3.84%)\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "view_data_distribution(voted_predict, \"bagging from csvs\", int_to_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1543706576485,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "8sJq9U5IiHW2",
    "outputId": "1ce70708-c7b7-463b-8437-ecdd3af2252d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save predict csv files of model: bagging to: submissions/submission_bagging.csv\n"
     ]
    }
   ],
   "source": [
    "# saving bagging submisions\n",
    "#average_accuracy = 1.0\n",
    "file_path = 'submissions/submission_' + \"bagging\"\n",
    "#file_path += '_{:.2f}'.format(100 * average_accuracy)\n",
    "file_path += '.csv'\n",
    "predict_to_csv(predict_bagging, file_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 394,
     "status": "ok",
     "timestamp": 1543706583819,
     "user": {
      "displayName": "Qiang YE",
      "photoUrl": "https://lh6.googleusercontent.com/-k-9mYNphfpc/AAAAAAAAAAI/AAAAAAAAARQ/zyID3sFntfM/s64/photo.jpg",
      "userId": "15200210150486633975"
     },
     "user_tz": 300
    },
    "id": "-Dht1O9StoMv",
    "outputId": "14c1d75f-f461-47ea-f892-9f04b977bf67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save predict csv files of model: Net5_0.5_0.0__YQ_0 to: submissions/submission_Net5_0.5_0.0__YQ_0.csv\n"
     ]
    }
   ],
   "source": [
    "# saving all single model submissions \n",
    "trainers = [\"YQ\"]\n",
    "for i, model in enumerate(models):\n",
    "    for trainer in trainers:\n",
    "        suffix = \"_\" + trainer + \"_\" + str(i)\n",
    "        current_model_name = model.get_name(suffix)\n",
    "        file_path = 'submissions/submission_' + current_model_name\n",
    "        #file_path += '_{:.2f}'.format(100 * average_accuracy)\n",
    "        file_path += '.csv'\n",
    "        predict_to_csv(all_predicts[:,i], file_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ALeTSZyT6Bpz"
   },
   "source": [
    "## All DONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7ax8e9oJGTa5"
   },
   "source": [
    "### Contribution\n",
    "\n",
    "**Jinfang Luo** designed and implemented `Linear SVM`,  and tested all the codes;\n",
    "\n",
    "**Lifeng Wan** designed and implemented class: `DFS`,  function: `get_region_size`, `get_point_in_biggest_region`,  `find_max_connected_points`,  `normalize_dataset`,  `predict_from_csv`,  `predicts_from_csvs`,  `lazy_bagging`,  and tested all the codes.\n",
    "\n",
    "**Qiang Ye** designed the architecture, implemented the rest codes, and wrote the documents and introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tbs1f2DtK01n"
   },
   "source": [
    "### License\n",
    "\n",
    "BSD"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Codes_HW4_IFT6390_Final.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
